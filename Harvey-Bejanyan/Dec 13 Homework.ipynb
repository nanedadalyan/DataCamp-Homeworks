{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                           // Added automatically on importing Spark\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Column\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.{DataType, DateType, TimestampType}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.DataFrameNaFunctions\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.DataFrameStatFunctions\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.sources._\n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@248073ec\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msc\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.3` // Or use any other 2.x version here\n",
    "import $ivy.`sh.almond::almond-spark:_` // Added automatically on importing Spark\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.Column\n",
    "import org.apache.spark.sql.types.{DataType, DateType, TimestampType}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.DataFrameNaFunctions\n",
    "import org.apache.spark.sql.DataFrameStatFunctions\n",
    "import org.apache.spark.sql.sources._\n",
    "\n",
    "\n",
    "var spark = SparkSession\n",
    ".builder()\n",
    ".appName(\"Spark HW 1\")\n",
    ".config(\"spark.master\", \"local\")\n",
    ".getOrCreate();\n",
    "\n",
    "def sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+\n",
      "|country|active_users in US|perc_of_total_users|\n",
      "+-------+------------------+-------------------+\n",
      "|     us|                 6|  6.122448979591836|\n",
      "+-------+------------------+-------------------+\n",
      "\n",
      "+-------+----------------------+-----------------+\n",
      "|country|editor_done_click >= 3|perc_of_US_users |\n",
      "+-------+----------------------+-----------------+\n",
      "|us     |4                     |66.66666666666666|\n",
      "+-------+----------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [device_id: string, country_code: string ... 13 more fields]\n",
       "\u001b[36mwindow_func\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@2b3aefad"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*******  Task 1  ********/\n",
    "\n",
    "val df = spark.read.\n",
    "option(\"header\", true).\n",
    "option(\"inferSchema\",true).\n",
    "csv(\"./SampledData.csv\").\n",
    "withColumn(\"date\", to_date(col(\"timestamp\")))\n",
    "\n",
    "val window_func = Window.partitionBy(\"device_id\").orderBy(\"timestamp\")\n",
    "\n",
    "df.select(\"device_id\", \"country_code\").\n",
    "groupBy(lower(col(\"country_code\")).alias(\"country\")).agg(countDistinct(\"device_id\").alias(\"active_users in US\")).\n",
    "filter(lower(col(\"country_code\")) === \"us\").\n",
    "withColumn(\"perc_of_total_users\", (col(\"active_users in US\")/df.select(\"device_id\").distinct.count)*100).\n",
    "show(100)\n",
    "\n",
    "df.select(\"device_id\", \"timestamp\", \"event_type\", \"country_code\").\n",
    "filter(col(\"event_type\") === \"editor_done_click\").\n",
    "filter(lower(col(\"country_code\")) === \"us\").\n",
    "withColumn(\"editor_done_click\", lead(\"timestamp\", 1) over window_func).\n",
    "groupBy(lower(col(\"country_code\")).alias(\"country\")).agg(countDistinct(\"device_id\").alias(\"editor_done_click >= 3\")).\n",
    "filter(col(\"editor_done_click >= 3\") >= 3).\n",
    "withColumn(\"perc_of_US_users\", col(\"editor_done_click >= 3\")/(df.select(\"device_id\").\n",
    "                                                         filter(lower(col(\"country_code\"))===\"us\")).distinct.count*100).\n",
    "orderBy(desc(\"perc_of_US_users\")).show(100, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------+\n",
      "|also_sharedT|users instagram shared|\n",
      "+------------+----------------------+\n",
      "|instagram   |28                    |\n",
      "+------------+----------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Accept-Encoding: string, advertising_id: string ... 67 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*******  Task 2  ********/\n",
    "\n",
    "val df = spark.read.\n",
    "option(\"header\", true).\n",
    "option(\"inferSchema\",true).\n",
    "csv(\"./photo_uploadTest.csv\")\n",
    "\n",
    "df.select(\"device_id\", \"also_sharedT\").\n",
    "groupBy(\"also_sharedT\").\n",
    "agg(countDistinct(\"device_id\").alias(\"users instagram shared\")).\n",
    "filter(col(\"also_sharedT\") === \"instagram\").\n",
    "show(100, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">df</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Dataset</span></span>[<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Row</span></span>] = [device_id: string, country_code: string ... 13 more fields]\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">installs</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">DataFrame</span></span> = [device_id: string, timestamp_ins: timestamp ... 14 more fields]\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">min</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span> = <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;2018-02-02&quot;</span></span>\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">max</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span> = <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">&quot;2018-02-18&quot;</span></span></code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [device_id: string, country_code: string ... 13 more fields]\n",
       "\u001b[36minstalls\u001b[39m: \u001b[32mDataFrame\u001b[39m = [device_id: string, timestamp_ins: timestamp ... 14 more fields]\n",
       "\u001b[36mmin\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"2018-02-02\"\u001b[39m\n",
       "\u001b[36mmax\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"2018-02-18\"\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/*******  Task 3  ********/\n",
    "\n",
    "val df = spark.read.\n",
    "option(\"header\", true).\n",
    "option(\"inferSchema\",true).\n",
    "csv(\"./SampledData.csv\").\n",
    "withColumn(\"date\", to_date(col(\"timestamp\"))).distinct\n",
    "\n",
    "val installs = spark.read.\n",
    "option(\"header\", true).\n",
    "option(\"inferSchema\",true).\n",
    "csv(\"./installsTest.csv\").\n",
    "withColumn(\"install_date\", to_date(col(\"timestamp_ins\")))\n",
    "\n",
    "var min = df.agg(min(col(\"date\"))).take(1)(0)(0).toString()\n",
    "var max = df.agg(max(col(\"date\"))).take(1)(0)(0).toString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|userType|cnt|\n",
      "+--------+---+\n",
      "|Old     |90 |\n",
      "|New     |8  |\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"device_id\").as(\"a\").\n",
    "join(installs.as(\"b\"), col(\"a.device_id\") === col(\"b.device_id\"),\"left\").\n",
    "withColumn(\"userType\",\n",
    "           when(col(\"b.device_id\").isNull,lit(\"Old\"))\n",
    "           when(col(\"b.device_id\").isNotNull && \n",
    "                to_date(col(\"timestamp_ins\")).between(min,max),lit(\"New\"))\n",
    "           otherwise(lit(\"Old\"))).\n",
    "groupBy(\"userType\").\n",
    "agg(countDistinct(\"a.device_id\").as(\"cnt\")).\n",
    "show(100,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept-Encoding: string (nullable = true)\n",
      " |-- advertising_id: string (nullable = true)\n",
      " |-- app: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- debug: boolean (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- is_following: boolean (nullable = true)\n",
      " |-- language_code: string (nullable = true)\n",
      " |-- market: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- platform: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- sid: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- sticker_id: long (nullable = true)\n",
      " |-- timezone: string (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- v: string (nullable = true)\n",
      " |-- version: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Accept-Encoding: string, advertising_id: string ... 22 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*******  Task 4  ********/\n",
    "\n",
    "val df = spark.read.\n",
    "option(\"header\", true).\n",
    "option(\"inferSchema\",true).\n",
    "csv(\"./sticker_save.csv\").\n",
    "withColumn(\"date\", to_date(col(\"timestamp\"))).distinct\n",
    "\n",
    "df.printSchema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [device_id: string, country_code: string ... 13 more fields]\n",
       "\u001b[36mbeautify\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Accept-Encoding: string, advertising_id: string ... 21 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*******  Task 5  ********/\n",
    "/** Face for overall Beautify ***/\n",
    "\n",
    "val df = spark.read.\n",
    "option(\"header\", true).\n",
    "option(\"inferSchema\",true).\n",
    "csv(\"./SampledData.csv\").\n",
    "withColumn(\"date\", to_date(col(\"timestamp\"))).distinct\n",
    "\n",
    "val beautify = spark.read.\n",
    "option(\"header\", true).\n",
    "option(\"inferSchema\",true).\n",
    "csv(\"./apply_beautify.csv\").\n",
    "withColumn(\"date\", to_date(col(\"timestamp\"))).distinct\n",
    "\n",
    "// df.select(\"event_type\").orderBy(\"event_type\").distinct.show(400, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------+\n",
      "|event_type|distinct face photo opens|\n",
      "+----------+-------------------------+\n",
      "|photo_open|15                       |\n",
      "+----------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Opens with face photos\n",
    "\n",
    "df.select(\"device_id\", \"event_type\").distinct.as(\"a\").\n",
    "join(beautify.as(\"b\"), col(\"a.device_id\") === col(\"b.device_id\"),\"left\").\n",
    "filter(col(\"a.event_type\") === \"photo_open\").\n",
    "groupBy(\"a.event_type\").\n",
    "agg(count(\"a.device_id\").as(\"distinct face photo opens\")).\n",
    "show(100, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|    tool|# of times used|\n",
      "+--------+---------------+\n",
      "|face_fix|           1491|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// count how many times face tool applied\n",
    "\n",
    "beautify.select(\"device_id\", \"tool\").\n",
    "filter(col(\"tool\") === \"face_fix\").\n",
    "groupBy(\"tool\").\n",
    "agg(count(\"device_id\").as(\"# of times used\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// change (%) in subscribed users' shares \n",
    "\n",
    "// How can we know if a user is subscribed or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*******  Task 6  ********/\n",
    "/** For subscribed users only ***/\n",
    "// Beautify Open -> Apply (conversion)\n",
    "// Stickiness\n",
    "// Engagement \n",
    "// Most used tools' combinations\n",
    "// Try -> Apply per tool comparison\n",
    "\n",
    "val df = spark.read.\n",
    "option(\"header\", true).\n",
    "option(\"inferSchema\",true).\n",
    "csv(\"./SampledData.csv\").\n",
    "withColumn(\"date\", to_date(col(\"timestamp\"))).distinct\n",
    "\n",
    "val beautify = spark.read.\n",
    "option(\"header\", true).\n",
    "option(\"inferSchema\",true).\n",
    "csv(\"./apply_beautify.csv\").\n",
    "withColumn(\"date\", to_date(col(\"timestamp\"))).distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------------+\n",
      "|event_type |editor_open -> apply_beautify|\n",
      "+-----------+-----------------------------+\n",
      "|editor_open|79                           |\n",
      "+-----------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Beautify Open -> Apply (conversion)\n",
    "df.select(\"device_id\", \"event_type\").distinct.as(\"a\").\n",
    "join(beautify.as(\"b\"), col(\"a.device_id\") === col(\"b.device_id\"), \"left\").\n",
    "filter(col(\"a.event_type\") === \"editor_open\").\n",
    "groupBy(\"a.event_type\").\n",
    "agg(count(\"a.device_id\").as(\"editor_open -> apply_beautify\")).\n",
    "show(100, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*******  Task 7  ********/\n",
    "/** Face Tool (before/after Multiface support) ***/\n",
    "// Face Open -> Offer View -> Trial -> Paid (conversions)\n",
    "// Frequency of applies\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
