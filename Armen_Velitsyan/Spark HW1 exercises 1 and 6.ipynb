{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T13:09:05.104145Z",
     "start_time": "2019-12-02T13:09:04.101Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "// import $ivy.`sh.almond::almond-spark:_` // Added automatically on importing Spark\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.3` // Or use any other 2.x version here\n",
    "// import $ivy.`sh.almond::almond-spark:_` // Added automatically on importing Spark\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T13:53:00.999636Z",
     "start_time": "2019-12-02T13:53:00.832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Column\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.{DataType, DateType, TimestampType}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "import org.apache.spark.sql.types.{DataType, DateType, TimestampType}\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T13:09:11.556699Z",
     "start_time": "2019-12-02T13:09:10.944Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@467c0788\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msc\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var spark = SparkSession\n",
    ".builder()\n",
    ".appName(\"Java Spark SQL basic example\")\n",
    ".config(\"spark.master\", \"local\")\n",
    ".getOrCreate();\n",
    "\n",
    "def sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T14:17:55.228522Z",
     "start_time": "2019-12-02T14:17:54.689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [device_id: string, country_code: string ... 12 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.option(\"header\", true).option(\"delimiter\", \",\").option(\"inferSchema\",true).csv(\"homeworkfile2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T13:29:09.892869Z",
     "start_time": "2019-12-02T13:29:09.775Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.Properties\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.DataFrame\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{DataFrame, SQLContext}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.{SparkConf, SparkContext}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{DataFrame, SparkSession}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SaveMode._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.json4s._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.json4s.jackson.JsonMethods._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\n",
       "\u001b[36mformatter\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mjoda\u001b[39m.\u001b[32mtime\u001b[39m.\u001b[32mformat\u001b[39m.\u001b[32mDateTimeFormatter\u001b[39m = org.joda.time.format.DateTimeFormatter@18142839\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Try\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.sql.{Connection, DriverManager, ResultSet}\n",
       "\u001b[39m\n",
       "\u001b[36msqlContext\u001b[39m: \u001b[32mSQLContext\u001b[39m = org.apache.spark.sql.SQLContext@6d55e699\n",
       "\u001b[32mimport \u001b[39m\u001b[36msqlContext.implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window._\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.joda.time.format.DateTimeFormat\n",
    "import org.joda.time.{DateTime, Days}\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import java.util.Properties\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.{DataFrame, SQLContext}\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.joda.time.{DateTime, Days}\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.SaveMode._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.json4s._\n",
    "import org.json4s.jackson.JsonMethods._\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import org.joda.time.{DateTime, Days}\n",
    "val formatter = DateTimeFormat.forPattern(\"yyyy-MM-dd\")\n",
    "import scala.util.Try\n",
    "import java.sql.{Connection, DriverManager, ResultSet}\n",
    "val sqlContext= new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlContext.implicits._\n",
    "import org.apache.spark.sql.expressions.Window._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT device_id)|\n",
      "+-------------------------+\n",
      "|                       98|\n",
      "+-------------------------+\n",
      "\n",
      "+-------------------------+\n",
      "|count(DISTINCT device_id)|\n",
      "+-------------------------+\n",
      "|                       94|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// 1. Calculate how many users who used app in X day \n",
    "// a) returned the next day\n",
    "// b) returned 7 day after\n",
    "\n",
    "\n",
    "// We have partitioned by the device_id and returned the next day (7 day after) of the date,\n",
    "// if there is no next date for that device, null is returned\n",
    "// then we have droped all the null values and count distinct device_id\n",
    "\n",
    "// returned the next day\n",
    "df.withColumn(\"date\",to_date(col(\"timestamp\")))\n",
    ".withColumn(\"day2\",lead(col(\"date\"),1).over(Window.partitionBy(\"device_id\").orderBy(\"date\"))).select(\"device_id\",\"date\",\"day2\")\n",
    ".na.drop().select(countDistinct(col(\"device_id\"))).show()\n",
    "\n",
    "// returned 7 day after\n",
    "df.withColumn(\"date\",to_date(col(\"timestamp\")))\n",
    ".withColumn(\"day2\",lead(col(\"date\"),7).over(Window.partitionBy(\"device_id\").orderBy(\"date\"))).select(\"device_id\",\"date\",\"day2\")\n",
    ".na.drop().select(countDistinct(col(\"device_id\"))).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------+-----------+-----+\n",
      "|5events                                                                                                       |NofSessions|Rate |\n",
      "+--------------------------------------------------------------------------------------------------------------+-----------+-----+\n",
      "|app_load,interstitial_ad_request,banner_ad_request,interstitial_ad_request,banner_ad_request                  |110        |0.095|\n",
      "|explore_open,interstitial_ad_request,interstitial_ad_request,app_load,native_ad_request                       |107        |0.093|\n",
      "|explore_open,app_load,interstitial_ad_request,banner_ad_request,interstitial_ad_request                       |54         |0.047|\n",
      "|app_load,interstitial_ad_request,editor_ad_request,interstitial_ad_request,banner_ad_request                  |32         |0.028|\n",
      "|explore_open,interstitial_ad_request,app_load,native_ad_request,native_ad_request                             |20         |0.017|\n",
      "|app_load,interstitial_ad_request,native_ad_request,interstitial_ad_request,editor_ad_request                  |14         |0.012|\n",
      "|interstitial_ad_waterfall,interstitial_ad_request,interstitial_ad_waterfall,interstitial_ad_request,app_load  |12         |0.01 |\n",
      "|explore_open,app_load,interstitial_ad_request,native_ad_request,interstitial_ad_request                       |12         |0.01 |\n",
      "|app_load,photo_choose_ad_request,interstitial_ad_request,native_ad_request,native_ad_request                  |10         |0.009|\n",
      "|imessage_search_click,imessage_maximize_page,imessage_search_click,imessage_sticker_use,imessage_minimize_page|10         |0.009|\n",
      "|explore_open,interstitial_ad_request,native_ad_request,native_ad_request,native_ad_request                    |8          |0.007|\n",
      "|explore_open,interstitial_ad_request,explore_close,interstitial_ad_request,app_load                           |7          |0.006|\n",
      "|explore_open,native_ad_request,interstitial_ad_request,interstitial_ad_request,app_load                       |6          |0.005|\n",
      "|interstitial_ad_request,interstitial_ad_request,native_ad_request,native_ad_request,native_ad_request         |6          |0.005|\n",
      "|explore_open,interstitial_ad_request,interstitial_ad_request,native_ad_request,native_ad_request              |6          |0.005|\n",
      "|explore_open,interstitial_ad_request,app_load,interstitial_ad_request,native_ad_request                       |6          |0.005|\n",
      "|app_open,explore_open,app_load,interstitial_ad_request,banner_ad_request                                      |5          |0.004|\n",
      "|interstitial_ad_request,banner_ad_request,banner_ad_request,app_load,native_ad_request                        |5          |0.004|\n",
      "|explore_open,interstitial_ad_request,interstitial_ad_request,app_load,banner_ad_request                       |5          |0.004|\n",
      "|explore_open,app_load,interstitial_ad_request,interstitial_ad_request,native_ad_request                       |5          |0.004|\n",
      "+--------------------------------------------------------------------------------------------------------------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [session_id: string, timestamp: timestamp ... 1 more field]\n",
       "\u001b[36mdf2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [session_id: string, 5events: string]\n",
       "\u001b[36mdf3\u001b[39m: \u001b[32mDataFrame\u001b[39m = [session_id: string, 5events: string]\n",
       "\u001b[36mdf4\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [5events: string, NofSessions: bigint]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//6)* what first 5 events users do in their session start (should be used window functions, find about it in the shared link above)\n",
    "// - 5 events in array and % of users did it\n",
    "\n",
    "\n",
    "// Select nescessary columns from the main DataFrame\n",
    "\n",
    "val df1 = df.select(\"session_id\",\"timestamp\",\"event_type\")\n",
    "\n",
    "// Create columns with lead function to order first 5 events in a session and concat in a list as seperate column\n",
    "val df2 = df1\n",
    ".withColumn(\"event_1\",lead(col(\"event_type\"),1).over(Window.partitionBy(\"session_id\").orderBy(\"timestamp\")))\n",
    ".withColumn(\"event_2\",lead(col(\"event_type\"),2).over(Window.partitionBy(\"session_id\").orderBy(\"timestamp\")))\n",
    ".withColumn(\"event_3\",lead(col(\"event_type\"),3).over(Window.partitionBy(\"session_id\").orderBy(\"timestamp\")))\n",
    ".withColumn(\"event_4\",lead(col(\"event_type\"),4).over(Window.partitionBy(\"session_id\").orderBy(\"timestamp\")))\n",
    ".withColumn(\"event_5\",lead(col(\"event_type\"),5).over(Window.partitionBy(\"session_id\").orderBy(\"timestamp\")))\n",
    ".withColumn(\"5events\",concat(col(\"event_1\"),lit(\",\"),col(\"event_2\"),lit(\",\"),col(\"event_3\"),lit(\",\"),col(\"event_4\"),lit(\",\"),col(\"event_5\")))\n",
    ".select(\"session_id\",\"5events\")\n",
    "\n",
    "// The first 5 events are presented as list in 5events column in first rows of each session\n",
    "// Here we choose the first row of each session and drop the remaining raws\n",
    "\n",
    " \n",
    "val df3 = df2\n",
    ".withColumn(\"rowId\", monotonically_increasing_id())\n",
    ".withColumn(\"rowId\", row_number.over(Window.partitionBy(col(\"session_id\"))\n",
    ".orderBy(col(\"rowId\").asc)))\n",
    ".where(col(\"rowId\") === 1).drop(\"rowId\").na.drop()\n",
    "\n",
    "\n",
    "//Groupby each sequence of 5 events and calculate the rate of appearance \n",
    "val df4 = df3.groupBy(\"5events\").agg(count(\"session_id\").as(\"NofSessions\")).sort(desc(\"NofSessions\"))\n",
    "\n",
    "df4.withColumn(\"Rate\",round(col(\"NofSessions\")/df4.select(sum(\"NofSessions\")).take(1)(0)(0),3)).show(false)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.11)",
   "language": "scala",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
