{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T13:09:05.104145Z",
     "start_time": "2019-12-02T13:09:04.101Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "// import $ivy.`sh.almond::almond-spark:_` // Added automatically on importing Spark\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.3` // Or use any other 2.x version here\n",
    "// import $ivy.`sh.almond::almond-spark:_` // Added automatically on importing Spark\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T13:53:00.999636Z",
     "start_time": "2019-12-02T13:53:00.832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Column\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.{DataType, DateType, TimestampType}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "import org.apache.spark.sql.types.{DataType, DateType, TimestampType}\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T13:09:11.556699Z",
     "start_time": "2019-12-02T13:09:10.944Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@2a7697d3\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msc\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var spark = SparkSession\n",
    ".builder()\n",
    ".appName(\"Java Spark SQL basic example\")\n",
    ".config(\"spark.master\", \"local\")\n",
    ".getOrCreate();\n",
    "\n",
    "def sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T14:17:55.228522Z",
     "start_time": "2019-12-02T14:17:54.689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [country_code: string, device_id: string ... 12 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.option(\"header\", true).option(\"delimiter\", \",\").option(\"inferSchema\",true).csv(\"homeworkfile.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T13:29:09.892869Z",
     "start_time": "2019-12-02T13:29:09.775Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.Properties\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.DataFrame\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{DataFrame, SQLContext}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.{SparkConf, SparkContext}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{DataFrame, SparkSession}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SaveMode._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.json4s._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.json4s.jackson.JsonMethods._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\n",
       "\u001b[36mformatter\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mjoda\u001b[39m.\u001b[32mtime\u001b[39m.\u001b[32mformat\u001b[39m.\u001b[32mDateTimeFormatter\u001b[39m = org.joda.time.format.DateTimeFormatter@78d3b14f\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Try\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.sql.{Connection, DriverManager, ResultSet}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.joda.time.format.DateTimeFormat\n",
    "import org.joda.time.{DateTime, Days}\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import java.util.Properties\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.{DataFrame, SQLContext}\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.joda.time.{DateTime, Days}\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.SaveMode._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.json4s._\n",
    "import org.json4s.jackson.JsonMethods._\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import org.joda.time.{DateTime, Days}\n",
    "val formatter = DateTimeFormat.forPattern(\"yyyy-MM-dd\")\n",
    "import scala.util.Try\n",
    "import java.sql.{Connection, DriverManager, ResultSet}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\n",
       "// In df1 a new \"date\" column is added and corresponding \"event_type\" is filtered in df (our basic DB of)\n",
       "\u001b[39m\n",
       "\u001b[36mdf1\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [country_code: string, device_id: string ... 13 more fields]\n",
       "\u001b[36mfiltereddf1\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [date: date, edit_item_open: bigint ... 3 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* 2. which tool has highest return rate (edit_text_apply, effect_apply, \n",
    "edit_item_open, edit_sticker_apply)\n",
    "*/\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "// In df1 a new \"date\" column is added and corresponding \"event_type\" is filtered in df (our basic DB of)\n",
    "var df1 =  df.withColumn(\"date\",to_date(col(\"timestamp\"))).where( col(\"event_type\") === \"edit_text_apply\" ||\n",
    "                      col(\"event_type\") === \"effect_apply\" ||\n",
    "                      col(\"event_type\") === \"edit_item_open\" ||\n",
    "                      col(\"event_type\") === \"edit_sticker_apply\")\n",
    "\n",
    "\n",
    "// In filtereddf1 we have grouped the df1 by date and pivoted event_types,\n",
    "// filtered the result and selected only  the first and last day of the period (where statement)\n",
    "\n",
    "var filtereddf1 = df1.groupBy(\"date\").pivot(\"event_type\").agg(countDistinct(col(\"device_id\")))\n",
    ".sort(\"date\").where(col(\"date\") === max(col(\"date\")) || col(\"date\") === min(col(\"date\")))\n",
    "\n",
    "// I have stuck here and cannot make operations for each row \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------------+\n",
      "|Day_of_week|avg(Number of photo_likes)|\n",
      "+-----------+--------------------------+\n",
      "|          1|                       2.5|\n",
      "|          2|                       4.5|\n",
      "|          3|                      3.25|\n",
      "|          4|                      1.75|\n",
      "|          5|                       3.4|\n",
      "|          6|                      3.25|\n",
      "|          7|                       3.0|\n",
      "+-----------+--------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfilteredDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [country_code: string, device_id: string ... 13 more fields]\n",
       "\u001b[36mgroupedDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Date: date, Number of photo_likes: bigint ... 1 more field]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*3. how much users did photo_like daily, in which day is it highest, if there is seasonality*/\n",
    "\n",
    "// Add date column to show date of event, filter df and left only rows with event “photo_like”\n",
    "var filteredDF = df.withColumn(\"Date\",to_date(col(\"timestamp\"))).filter(col(\"event_type\") === \"photo_like\")\n",
    "\n",
    "//group by date filteredDF and add column “Day of week”  and check weekly seasonality\n",
    "var groupedDF = filteredDF.groupBy(\"Date\").agg(count(\"event_type\").alias(\"Number of photo_likes\")).withColumn(\"Day_of_week\", date_format(col(\"Date\"), \"u\")).sort(\"Date\")\n",
    "\n",
    "groupedDF.groupBy(\"Day_of_week\").agg(avg(\"Number of photo_likes\")).sort(\"Day_of_week\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+------------------+\n",
      "|Number of effect_try|Number of effect_apply|Conversion rate(%)|\n",
      "+--------------------+----------------------+------------------+\n",
      "|               16141|                  2019| 12.50851867914008|\n",
      "+--------------------+----------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36meff_try\u001b[39m: \u001b[32mColumn\u001b[39m = count(CASE WHEN (event_type = effect_try) THEN true END)\n",
       "\u001b[36meff_apply\u001b[39m: \u001b[32mColumn\u001b[39m = count(CASE WHEN (event_type = effect_apply) THEN true END)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*4. what is conversion rate that users who did effect_try then did effect apply*/\n",
    "// Count the number of “effect_try” in event_type column – eff_try\n",
    "\n",
    "var eff_try = count(when(col(\"event_type\") === \"effect_try\",true))\n",
    "\n",
    "// Count the number of “effect_apply” in event_type column – eff_apply\n",
    "var eff_apply = count(when(col(\"event_type\") === \"effect_apply\",true))\n",
    "\n",
    "//Calculate conversion ratio by initially dropping all the null values from “user_id” column and show results\n",
    "df.na.drop(\"any\",Seq(\"user_id\")).select(eff_try as \"Number of effect_try\",eff_apply as \"Number of effect_apply\" ,eff_apply/eff_try*100 as \"Conversion rate(%)\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------+\n",
      "|summary|Number of effect_applies|\n",
      "+-------+------------------------+\n",
      "|  count|                      29|\n",
      "|   mean|      128.06896551724137|\n",
      "| stddev|      19.033675109587744|\n",
      "|    min|                      97|\n",
      "|    max|                     179|\n",
      "+-------+------------------------+\n",
      "\n",
      "+----------+-----------+------------------------+\n",
      "|      Date|Day_of_week|Number of effect_applies|\n",
      "+----------+-----------+------------------------+\n",
      "|2018-02-08|          4|                     100|\n",
      "|2018-02-11|          7|                     179|\n",
      "|2018-02-23|          5|                      97|\n",
      "+----------+-----------+------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mnewDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [country_code: string, device_id: string ... 14 more fields]\n",
       "\u001b[36mgroupedDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Date: date, Day_of_week: string ... 1 more field]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*5. find how many users did effect apply and what is descriptive statistics for it (outliers, avg, median)*/\n",
    "\n",
    "// Add 2 columns \"Date\" and \"Day_of_week\" to our dataFrame - newDF\n",
    "var newDF = df.withColumn(\"Date\",to_date(col(\"timestamp\"))).withColumn(\"Day_of_week\",date_format(col(\"Date\"),\"u\"))\n",
    "\n",
    "// Create group newDF by  \"Date\" and \"Day_of_week\" columns aggregating with \"event_type\"\n",
    "// where \"event_type\" is \"effect_apply\" and dropping all Null values genereted by cube method\n",
    "var groupedDF = newDF.cube(\"Date\",\"Day_of_week\").agg(count(when(col(\"event_type\") === \"effect_apply\",true)) as \"Number of effect_applies\").na.drop().sort(\"Date\")\n",
    "\n",
    "// Generate general statistic  by describe method\n",
    "groupedDF.describe(\"Number of effect_applies\").show()\n",
    "\n",
    "// Assuming that outliers here are \"mean - 2*stddev\" < row < \"mean + 2*stddev\" \n",
    "groupedDF.where(col(\"Number of effect_applies\") > \n",
    "                (mean(col(\"Number of effect_applies\")) + 2*(stddev(col(\"Number of effect_applies\"))))\n",
    "                 || col(\"Number of effect_applies\") < (mean(col(\"Number of effect_applies\")\n",
    "                    - 2*(stddev(col(\"Number of effect_applies\")))))).show()\n",
    "\n",
    "//We have one outlier on 2018-02-11 (sunday) with 189 used effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* 7 What are the most important metrics when we want to describe one session of user in app \n",
    "\n",
    "Depends on what we mean by saying important, for data scienteis more important metrics to my opinion are:\n",
    "\n",
    "1. Duration of the session \n",
    "2. What events were created (i.e. tools used) by user during session\n",
    "3. Sequence of events during a session\n",
    "4. The opportunity to place the session in any cluster (hope you have any :), otherwise think of it)\n",
    "5. How many events were completed\n",
    "\n",
    "\n",
    "\n",
    "*/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.11)",
   "language": "scala",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
