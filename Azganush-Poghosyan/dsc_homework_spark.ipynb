{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.Properties\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.DataFrame\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{DataFrame, SQLContext}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.{SparkConf, SparkContext}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{DataFrame, SparkSession}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SaveMode._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.json4s._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.json4s.jackson.JsonMethods._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\r\n",
       "\u001b[36mformatter\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mjoda\u001b[39m.\u001b[32mtime\u001b[39m.\u001b[32mformat\u001b[39m.\u001b[32mDateTimeFormatter\u001b[39m = org.joda.time.format.DateTimeFormatter@2bcd9a5a\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Try\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.sql.{Connection, DriverManager, ResultSet}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkContext  \n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Column\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.{DataType, DateType, TimestampType}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\r\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@3faf5757\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msc\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.3` \n",
    "import org.apache.spark.sql._\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import org.joda.time.{DateTime, Days}\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import java.util.Properties\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.{DataFrame, SQLContext}\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.joda.time.{DateTime, Days}\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.SaveMode._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.json4s._\n",
    "import org.json4s.jackson.JsonMethods._\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import org.joda.time.{DateTime, Days}\n",
    "val formatter = DateTimeFormat.forPattern(\"yyyy-MM-dd\")\n",
    "import scala.util.Try\n",
    "import java.sql.{Connection, DriverManager, ResultSet}\n",
    "import org.apache.spark.SparkContext  \n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.Column\n",
    "import org.apache.spark.sql.types.{DataType, DateType, TimestampType}\n",
    "import org.apache.spark.sql.functions._\n",
    "val spark = {\n",
    "  SparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "def sc = spark.sparkContext\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- language_code: string (nullable = true)\n",
      " |-- platform: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mframe\u001b[39m: \u001b[32mDataFrame\u001b[39m = [device_id: string, country_code: string ... 12 more fields]\r\n",
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [device_id: string, country_code: string ... 8 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val frame = spark.read.option(\"header\", true).option(\"inferSchema\",true).csv(\"picsart_2.csv\")\n",
    "val df = frame.drop(\"advertising_id\", \"timezone\", \"duration\", \"app\", \"v\").withColumn(\"date\", frame(\"timestamp\").cast(DateType))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+--------+\n",
      "|      date|count_id_set|count_ret_ids|ret_rate|\n",
      "+----------+------------+-------------+--------+\n",
      "|2018-02-02|          94|           37|   39.36|\n",
      "|2018-02-03|          41|           23|    56.1|\n",
      "|2018-02-04|          39|           25|    64.1|\n",
      "|2018-02-05|          45|           24|   53.33|\n",
      "|2018-02-06|          35|           22|   62.86|\n",
      "|2018-02-07|          34|           20|   58.82|\n",
      "|2018-02-08|          36|           15|   41.67|\n",
      "|2018-02-09|          29|           16|   55.17|\n",
      "|2018-02-10|          31|           17|   54.84|\n",
      "|2018-02-11|          28|           19|   67.86|\n",
      "|2018-02-12|          33|           23|    69.7|\n",
      "|2018-02-13|          29|           22|   75.86|\n",
      "|2018-02-14|          40|           19|    47.5|\n",
      "|2018-02-15|          33|           17|   51.52|\n",
      "|2018-02-16|          30|           10|   33.33|\n",
      "|2018-02-17|          20|            2|    10.0|\n",
      "|2018-02-18|           2|           -1|   -50.0|\n",
      "+----------+------------+-------------+--------+\n",
      "\n",
      "+------------------+\n",
      "|     avg(ret_rate)|\n",
      "+------------------+\n",
      "|52.626250000000006|\n",
      "+------------------+\n",
      "\n",
      "+----------+------------+-------------+--------+\n",
      "|      date|count_id_set|count_ret_ids|ret_rate|\n",
      "+----------+------------+-------------+--------+\n",
      "|2018-02-02|           8|            2|    25.0|\n",
      "|2018-02-03|           6|            2|   33.33|\n",
      "|2018-02-04|           5|            2|    40.0|\n",
      "|2018-02-05|           5|            1|    20.0|\n",
      "|2018-02-06|           2|            1|    50.0|\n",
      "|2018-02-07|           4|            2|    50.0|\n",
      "|2018-02-08|           5|            1|    20.0|\n",
      "|2018-02-09|           3|            1|   33.33|\n",
      "|2018-02-10|           5|            1|    20.0|\n",
      "|2018-02-11|           2|            0|     0.0|\n",
      "|2018-02-12|           1|            0|     0.0|\n",
      "|2018-02-13|           3|            1|   33.33|\n",
      "|2018-02-14|           3|            1|   33.33|\n",
      "|2018-02-15|           3|            0|     0.0|\n",
      "|2018-02-16|           4|            0|     0.0|\n",
      "|2018-02-17|           1|           -1|  -100.0|\n",
      "+----------+------------+-------------+--------+\n",
      "\n",
      "+------------------+\n",
      "|     avg(ret_rate)|\n",
      "+------------------+\n",
      "|23.887999999999995|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mRetention\u001b[39m\r\n",
       "\u001b[36mret_1\u001b[39m: \u001b[32mRetention\u001b[39m = ammonite.$sess.cmd10$Helper$Retention@6e681b8f"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**Retention object that takes the following arguments: dataframe, date column, id column, and an integer as the number \n",
    "of days. Has two methods: 1) creates a dataframe returning each day's number of users, the number of users that returned back \n",
    "after the number of days indicated in the arguments, and the retention rate, 2) ret_by_event method which takes extra\n",
    "arguments: the filtering column name and the exact filtering value, to return the retention rate of only that event's users.\n",
    "*/\n",
    "\n",
    "class Retention (frame:DataFrame, col_date:String, col_id:String, days:Int) {\n",
    "def day_retention():DataFrame= {\n",
    "  var df_1 = frame.select(col_date, col_id)\n",
    "  var df_2 = df_1.groupBy(col_date).agg(collect_set(col(col_id)).as(\"id_set\"))\n",
    "  var df_join = df_1.join(df_2, df_1(col_date)<=>df_2(col_date), \"inner\").drop(df_1(col_date)).\n",
    "                groupBy(col(col_date), col(\"id_set\")).agg(countDistinct(col(col_id)).as(\"count_id_set\")).sort(col(col_date)) \n",
    "  var window_spec = Window.orderBy(col_date)\n",
    "  var df_join_added = df_join.withColumn(\"id_set_days\", lead(\"id_set\", 1) over window_spec)\n",
    "  var df_join_added_1 = df_join_added.withColumn(\"common_id_set\", array_intersect(col(\"id_set\"), col(\"id_set_days\")))\n",
    "  var df_final = df_join_added_1.withColumn(\"count_ret_ids\", size(col(\"common_id_set\"))).withColumn(\"ret_rate\", \n",
    "                 round(col(\"count_ret_ids\")*100/col(\"count_id_set\"), 2)).drop(\"id_set\", \"id_set_days\", \"common_id_set\")\n",
    "  df_final\n",
    "}\n",
    "def ret_by_event(col_event:String, event_type:String): DataFrame={\n",
    "  var df_1 = frame.filter(col(col_event)===event_type).select(col_date, col_id)\n",
    "  var df_2 = df_1.groupBy(col_date).agg(collect_set(col(col_id)).as(\"id_set\"))\n",
    "  var df_join = df_1.join(df_2, df_1(col_date)<=>df_2(col_date), \"inner\").drop(df_1(col_date)).\n",
    "   groupBy(col(col_date), col(\"id_set\")).agg(countDistinct(col(col_id)).as(\"count_id_set\")).sort(col(col_date)) \n",
    "  var window_spec = Window.orderBy(col_date)\n",
    "  var df_join_added = df_join.withColumn(\"id_set_days\", lead(\"id_set\", 1) over window_spec)\n",
    "  var df_join_added_1 = df_join_added.withColumn(\"common_id_set\", array_intersect(col(\"id_set\"), col(\"id_set_days\")))\n",
    "  var df_final = df_join_added_1.withColumn(\"count_ret_ids\", size(col(\"common_id_set\"))).withColumn(\"ret_rate\", \n",
    "                 round(col(\"count_ret_ids\")*100/col(\"count_id_set\"), 2)).drop(\"id_set\", \"id_set_days\", \"common_id_set\")\n",
    "                        \n",
    "  df_final\n",
    "}\n",
    " \n",
    "}\n",
    "//1.Calculate how many users who used app in X day:  a) returned the next day  b) returned 7 day after\n",
    "val ret_1 = new Retention(df, \"date\", \"device_id\", 1)  //b) input 7 instead of 1 in the arguments\n",
    "ret_1.day_retention.show()\n",
    "ret_1.day_retention.filter(col(\"date\")=!=\"2018-02-18\").agg(avg(col(\"ret_rate\"))).show()\n",
    "//2.which tool has highest return rate (edit_text_apply, effect_apply, edit_item_open, edit_sticker_apply)\n",
    "ret_1.ret_by_event(\"event_type\", \"edit_text_apply\").show()\n",
    "ret_1.ret_by_event(\"event_type\", \"edit_text_apply\").filter(col(\"date\")=!=\"2018-02-17\").agg(avg(col(\"ret_rate\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mGroupCount\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfind_avg\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfind_avg_group\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfind_max\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfind_max_group\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfind_min\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfind_min_group\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**functions to group_by and count, find average, min, max of a column, and the same for each group*/\n",
    "def GroupCount(frame:DataFrame, groupCols:Array[String], countCols:Array[String]):DataFrame = {\n",
    "    var groupCols1:Array[Column] = groupCols.map(x=>col(x))\n",
    "    var countCols1:Array[Column] = countCols.map(c => countDistinct(col(c)).alias(c))\n",
    "    var dfFiltered = frame.groupBy(groupCols.head, groupCols.tail : _*).agg(countCols1.head, countCols1.tail : _*)\n",
    "    dfFiltered\n",
    "    }\n",
    "def find_avg(frame:DataFrame, avg_col:String):DataFrame = {\n",
    "    var dfAvgCol = frame.agg(round(avg(col(avg_col)), 2))\n",
    "    dfAvgCol\n",
    "  }\n",
    "def find_avg_group(frame:DataFrame, group_col:String, avg_col:String) = {\n",
    "    var dfAvgColGroup = frame.groupBy(col(group_col)).agg(round(avg(col(avg_col)), 2))\n",
    "    dfAvgColGroup\n",
    "}\n",
    "def find_max(frame:DataFrame, max_col:String):DataFrame = {\n",
    "     var dfMaxCol = frame.agg(max(col(max_col)))\n",
    "     dfMaxCol\n",
    "  }\n",
    "def find_max_group(frame:DataFrame, group_col:String, max_col:String) = {\n",
    "    var dfMaxColGroup = frame.groupBy(col(group_col)).agg(max(col(max_col)))\n",
    "    dfMaxColGroup\n",
    "}\n",
    "def find_min(frame:DataFrame, min_col:String):DataFrame = {\n",
    "     var dfMinCol = frame.agg(min(col(min_col)))\n",
    "     dfMinCol\n",
    "  }\n",
    "def find_min_group(frame:DataFrame, group_col:String, min_col:String) = {\n",
    "    var dfMinColGroup = frame.groupBy(col(group_col)).agg(min(col(min_col)))\n",
    "    dfMinColGroup\n",
    "}\n",
    "\n",
    "//3) a) how much users did photo_like daily, b) in which day is it highest, c) if there is seasonality\n",
    "//val df_new = df.filter(col(\"event_type\")===\"photo_like\").withColumn(\"week_day\", date_format(col(\"timestamp\"), \"u\")) \n",
    "//a)\n",
    "//val df_1 = GroupCount(df_new, Array(\"week_day\", \"date\"), Array(\"device_id\")).filter(col(\"date\")=!=\"2018-02-02\").sort(\"date\")\n",
    "//df_1.show(50)\n",
    "//find_avg_group(df_1, \"week_day\", \"device_id\").sort(col(\"week_day\")).show()\n",
    "//b) in Feb 2018 on average on Tuesday users were more active in liking photos than on any other day.\n",
    "//c)on Saturdays and Sundays, on average, more users did \"photo_like\", especially on Sunday because our data range contains\n",
    "//one Sunday less than any other weekday. And on Mondays definitely the average number of users doing \"photo_like\" \n",
    "//is apparently low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|          event_type|       session_id|\n",
      "+--------------------+-----------------+\n",
      "|          effect_try|              298|\n",
      "|effect_try , effe...|              243|\n",
      "|     conversion_rate|81.54362416107382|\n",
      "+--------------------+-----------------+\n",
      "\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mconversion_rate\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//4) what is conversion rate that users who did effect_try then did effect apply\n",
    " def conversion_rate(frame:DataFrame, event_col:String, stage1:String, stage2:String, id_col:String, \n",
    "                     time_col:String):Unit = {\n",
    "     val window_spec = Window.partitionBy(id_col).orderBy(time_col)\n",
    "     val count_stage1 = GroupCount(frame.select(id_col, time_col, event_col).filter(col(event_col)=== stage1),\n",
    "                        Array(event_col), Array(id_col))\n",
    "     val df_event = frame.select(id_col, time_col, event_col).filter(col(event_col)=== stage1 || col(event_col)===stage2).\n",
    "                    withColumn(\"lead\", lead(col(event_col), 1) over window_spec).filter(col(event_col)===stage1).\n",
    "                    withColumn(\"concat_col\", concat(col(event_col),lit(\" , \"),col(\"lead\")))\n",
    "     val count_stage2 = GroupCount(df_event.filter(col(\"concat_col\")===stage1+\" , \"+stage2), Array(\"concat_col\"),Array(id_col))\n",
    "     val count_union = count_stage1.union(count_stage2)\n",
    "     val new_df = count_union.union(List((\"conversion_rate\", (count_stage2.select(id_col).\n",
    "                take(1)(0)(0).toString.toDouble*100/count_stage1.select(id_col).take(1)(0)(0).toString.toInt).toString))\n",
    "                                        .toDF)\n",
    "     println(new_df.show())\n",
    " }\n",
    "conversion_rate(df, \"event_type\", \"effect_try\", \"effect_apply\", \"session_id\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|  count_device_id|\n",
      "+-------+-----------------+\n",
      "|  count|               17|\n",
      "|   mean|8.411764705882353|\n",
      "| stddev|4.169214907051023|\n",
      "|    min|                1|\n",
      "|    max|               20|\n",
      "+-------+-----------------+\n",
      "\n",
      "()\n",
      "Q1: 7.0\n",
      "median: 8.0\n",
      "Q3: 9.0\n",
      "+------------+--------+\n",
      "|outlier_date|outliers|\n",
      "+------------+--------+\n",
      "|  2018-02-04|      14|\n",
      "|  2018-02-02|      20|\n",
      "|  2018-02-17|       4|\n",
      "|  2018-02-18|       1|\n",
      "+------------+--------+\n",
      "\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">df_5</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">DataFrame</span></span> = [event_type: string, date: date ... 1 more field]\r\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">df_5_short</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">DataFrame</span></span> = [date: date, count_device_id: bigint]\r\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">description</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">DataFrame</span></span> = [summary: string, count_device_id: string]\r\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">quantiles</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Array</span></span>[<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Double</span></span>] = <span style=\"color: yellow\"><span class=\"ansi-yellow-fg\">Array</span></span>(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">7.0</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">8.0</span></span>, <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">9.0</span></span>)\r\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">Q1</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Double</span></span> = <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">7.0</span></span>\r\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">median</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Double</span></span> = <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">8.0</span></span>\r\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">Q3</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Double</span></span> = <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">9.0</span></span>\r\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">IQR</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Double</span></span> = <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">2.0</span></span>\r\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">lower_outlier_range</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Double</span></span> = <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">4.0</span></span>\r\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">upper_outlier_range</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Double</span></span> = <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">12.0</span></span>\r\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">outliers</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">DataFrame</span></span> = [outlier_date: date, outliers: bigint]</code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mdf_5\u001b[39m: \u001b[32mDataFrame\u001b[39m = [event_type: string, date: date ... 1 more field]\r\n",
       "\u001b[36mdf_5_short\u001b[39m: \u001b[32mDataFrame\u001b[39m = [date: date, count_device_id: bigint]\r\n",
       "\u001b[36mdescription\u001b[39m: \u001b[32mDataFrame\u001b[39m = [summary: string, count_device_id: string]\r\n",
       "\u001b[36mquantiles\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m7.0\u001b[39m, \u001b[32m8.0\u001b[39m, \u001b[32m9.0\u001b[39m)\r\n",
       "\u001b[36mQ1\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m7.0\u001b[39m\r\n",
       "\u001b[36mmedian\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m8.0\u001b[39m\r\n",
       "\u001b[36mQ3\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m9.0\u001b[39m\r\n",
       "\u001b[36mIQR\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m2.0\u001b[39m\r\n",
       "\u001b[36mlower_outlier_range\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m4.0\u001b[39m\r\n",
       "\u001b[36mupper_outlier_range\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m12.0\u001b[39m\r\n",
       "\u001b[36moutliers\u001b[39m: \u001b[32mDataFrame\u001b[39m = [outlier_date: date, outliers: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//5) find how many users did effect apply and what is descriptive statistics for it (outliers, avg, median)\n",
    "val df_5 = df.filter(col(\"event_type\")===\"effect_apply\").select(\"event_type\", \"date\", \"device_id\")\n",
    "val df_5_short = GroupCount(df_5, Array(\"date\"), Array(\"device_id\")).withColumnRenamed(\"device_id\", \"count_device_id\")\n",
    "//count, avg, median, outliers\n",
    "var description = df_5_short.describe()\n",
    "var quantiles = df_5_short.stat.approxQuantile(\"count_device_id\", Array(0.25, 0.5, 0.75), 0.0)\n",
    "var Q1 = quantiles(0)\n",
    "var median = quantiles(1)\n",
    "var Q3 = quantiles(2)\n",
    "var IQR = Q3-Q1\n",
    "var lower_outlier_range = Q1 - 1.5*IQR\n",
    "var upper_outlier_range = Q3 + 1.5*IQR\n",
    "var outliers = df_5_short.filter(col(\"count_device_id\")<=lower_outlier_range || col(\"count_device_id\")>= \n",
    "               upper_outlier_range).withColumnRenamed(\"date\", \"outlier_date\").withColumnRenamed(\"count_device_id\",\n",
    "               \"outliers\")\n",
    "println(description.show())\n",
    "println(\"Q1: \"+Q1+\"\\n\"+\"median: \"+median+\"\\n\"+\"Q3: \"+Q3)\n",
    "println(outliers.show(30))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|              events|percent_users|\n",
      "+--------------------+-------------+\n",
      "|                null|        100.0|\n",
      "|interstitial_ad_r...|         9.41|\n",
      "|banner_ad_request...|         7.47|\n",
      "|interstitial_ad_r...|         7.42|\n",
      "|banner_ad_request...|         7.42|\n",
      "|app_load,intersti...|         7.42|\n",
      "|explore_open,app_...|         7.21|\n",
      "|native_ad_waterfa...|         6.41|\n",
      "|native_ad_respons...|         6.41|\n",
      "|interstitial_ad_r...|         5.15|\n",
      "|native_ad_respons...|          5.1|\n",
      "|app_open,explore_...|         5.02|\n",
      "|native_ad_request...|         5.02|\n",
      "|explore_open,inte...|         4.85|\n",
      "|shop_response,sho...|         4.81|\n",
      "|effect_try,effect...|         4.72|\n",
      "|banner_ad_request...|         4.39|\n",
      "|native_ad_respons...|         4.26|\n",
      "|shop_request,shop...|         4.22|\n",
      "|shop_response,sho...|         4.01|\n",
      "+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">window</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">expressions</span></span>.<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">WindowSpec</span></span> = org.apache.spark.sql.expressions.WindowSpec@665c3850</code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mwindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@665c3850"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//6)* what first 5 events users do in their session start (should be used window functions, \n",
    "//find about it in the shared link above) - 5 events in array and % of users did it\n",
    "//No general function yet.\n",
    "var window = Window.partitionBy(\"session_id\").orderBy(\"timestamp\")\n",
    "df.\n",
    "withColumn(\"ev_1\", lead(\"event_type\", 1) over window).\n",
    "withColumn(\"ev_2\", lead(\"event_type\", 2) over window).\n",
    "withColumn(\"ev_3\", lead(\"event_type\", 3) over window).\n",
    "withColumn(\"ev_4\", lead(\"event_type\", 4) over window).\n",
    "withColumn(\"events\",concat(col(\"event_type\"),lit(\",\"),col(\"ev_1\"),lit(\",\"),col(\"ev_2\"),lit(\",\"),col(\"ev_3\"),lit(\",\"),\n",
    "        col(\"ev_4\"))).select(\"session_id\",\"events\").groupBy(\"events\").agg(countDistinct(\"session_id\").as(\"count_5s\")).\n",
    "        orderBy(col(\"count_5s\").desc).withColumn(\"percent_users\",round(col(\"count_5s\")/df.agg(countDistinct(\"session_id\")).\n",
    "        take(1)(0)(0).toString.toInt*100,2)).drop(\"count_5s\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msession_duration\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmost_popular_ten_events\u001b[39m"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//7)* What are the most important metrics when we want to describe one session of user in app (edited)\n",
    "//-session duration \n",
    "  \n",
    "def session_duration(dataframe:DataFrame, session_id:String):Double = {\n",
    "    var session_id_col = dataframe.filter(col(\"session_id\")===session_id)\n",
    "    var time_col = dataframe(\"timestamp\")\n",
    "    var new_frame = dataframe.groupBy(col(\"session_id\")).agg(min(col(\"timestamp\")).as(\"session_start\"),\n",
    "                    max(time_col).as(\"session_end\")).withColumn(\"duration_seconds\", unix_timestamp\n",
    "                    (col(\"session_end\"))-unix_timestamp(col(\"session_start\")))\n",
    "    var session_duration = new_frame.take(1)(0)(0).toString.toDouble\n",
    "    session_duration\n",
    "}\n",
    "//-conversion rate of \"event_try\"/\"event_apply\"  //conversion function\n",
    "//-ranking of most used events\n",
    "def most_popular_ten_events(dataframe:DataFrame, session_id:String):Unit = {\n",
    "        var frame_1 = dataframe.filter(col(\"session_id\")===session_id)\n",
    "        var frame_2 = frame_1.groupBy(\"event_type\").agg(countDistinct(col(\"session_id\"))).sort(desc(\"session_id\")).limit(10)\n",
    "        println(frame_2.show(false))\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

