{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.Properties\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.DataFrame\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{DataFrame, SQLContext}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.{SparkConf, SparkContext}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{DataFrame, SparkSession}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SaveMode._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.json4s._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.json4s.jackson.JsonMethods._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\r\n",
       "\u001b[36mformatter\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mjoda\u001b[39m.\u001b[32mtime\u001b[39m.\u001b[32mformat\u001b[39m.\u001b[32mDateTimeFormatter\u001b[39m = org.joda.time.format.DateTimeFormatter@12f7609\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Try\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.sql.{Connection, DriverManager, ResultSet}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkContext  \n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Column\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.{DataType, DateType, TimestampType}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\r\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@febfe06\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msc\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.3` \n",
    "import org.apache.spark.sql._\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import org.joda.time.{DateTime, Days}\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import java.util.Properties\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.{DataFrame, SQLContext}\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.joda.time.{DateTime, Days}\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.SaveMode._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.json4s._\n",
    "import org.json4s.jackson.JsonMethods._\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import org.joda.time.{DateTime, Days}\n",
    "val formatter = DateTimeFormat.forPattern(\"yyyy-MM-dd\")\n",
    "import scala.util.Try\n",
    "import java.sql.{Connection, DriverManager, ResultSet}\n",
    "import org.apache.spark.SparkContext  \n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.Column\n",
    "import org.apache.spark.sql.types.{DataType, DateType, TimestampType}\n",
    "import org.apache.spark.sql.functions._\n",
    "val spark = {\n",
    "  SparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "def sc = spark.sparkContext\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- language_code: string (nullable = true)\n",
      " |-- platform: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n",
      "+---------------------------+\n",
      "|event_type                 |\n",
      "+---------------------------+\n",
      "|app_load                   |\n",
      "|native_ad_request          |\n",
      "|native_ad_waterfall        |\n",
      "|native_ad_waterfall        |\n",
      "|banner_ad_response         |\n",
      "|interstitial_ad_waterfall  |\n",
      "|app_load                   |\n",
      "|native_ad_request          |\n",
      "|card_view                  |\n",
      "|editor_ad_response         |\n",
      "|interstitial_ad_response   |\n",
      "|native_ad_response         |\n",
      "|banner_ad_response         |\n",
      "|banner_ad_response         |\n",
      "|card_view                  |\n",
      "|interstitial_ad_request    |\n",
      "|interstitial_ad_response   |\n",
      "|edit_sticker_category_open |\n",
      "|tags_widget_shown          |\n",
      "|shop_tab_change            |\n",
      "|edit_frame_try             |\n",
      "|collage_grid_apply         |\n",
      "|editor_ad_request          |\n",
      "|editor_ad_response         |\n",
      "|interstitial_ad_request    |\n",
      "|photo_upload               |\n",
      "|explore_open               |\n",
      "|banner_ad_response         |\n",
      "|native_ad_response         |\n",
      "|photo_chooser_photo_click  |\n",
      "|draw_open                  |\n",
      "|tags_widget_shown          |\n",
      "|shop_request               |\n",
      "|shop_request               |\n",
      "|editor_ad_response         |\n",
      "|native_ad_response         |\n",
      "|banner_item_view           |\n",
      "|tabbar_click               |\n",
      "|native_ad_waterfall        |\n",
      "|card_view                  |\n",
      "|native_ad_response         |\n",
      "|native_ad_response         |\n",
      "|native_ad_request          |\n",
      "|native_ad_response         |\n",
      "|app_load                   |\n",
      "|editor_ad_request          |\n",
      "|editor_ad_response         |\n",
      "|shop_response              |\n",
      "|edit_top_menu_item_click   |\n",
      "|interstitial_ad_request    |\n",
      "|shop_request               |\n",
      "|shop_response              |\n",
      "|interstitial_ad_request    |\n",
      "|native_ad_waterfall        |\n",
      "|edit_top_menu_item_click   |\n",
      "|native_ad_view             |\n",
      "|photo_view                 |\n",
      "|card_view                  |\n",
      "|banner_item_view           |\n",
      "|native_ad_waterfall        |\n",
      "|onboarding_request         |\n",
      "|interstitial_ad_request    |\n",
      "|editor_open                |\n",
      "|shop_response              |\n",
      "|shop_response              |\n",
      "|interstitial_ad_waterfall  |\n",
      "|shop_response              |\n",
      "|interstitial_ad_waterfall  |\n",
      "|native_ad_response         |\n",
      "|shop_request               |\n",
      "|native_ad_waterfall        |\n",
      "|edit_item_open             |\n",
      "|shop_load                  |\n",
      "|shop_exit                  |\n",
      "|edit_sticker_category_open |\n",
      "|experiment_participate     |\n",
      "|shop_package_open          |\n",
      "|edit_sticker_category_open |\n",
      "|page_scroll                |\n",
      "|shop_package_open          |\n",
      "|interstitial_ad_request    |\n",
      "|banner_ad_request          |\n",
      "|shop_request               |\n",
      "|interstitial_ad_request    |\n",
      "|banner_ad_response         |\n",
      "|native_ad_waterfall        |\n",
      "|rewarded_video_ad_response |\n",
      "|shop_response              |\n",
      "|shop_response              |\n",
      "|shop_response              |\n",
      "|interstitial_ad_waterfall  |\n",
      "|search_result_seen         |\n",
      "|editor_open                |\n",
      "|editor_open                |\n",
      "|banner_ad_request          |\n",
      "|native_ad_response         |\n",
      "|app_open                   |\n",
      "|edit_item_open             |\n",
      "|effect_try                 |\n",
      "|draw_shape_settings_changed|\n",
      "+---------------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mframe\u001b[39m: \u001b[32mDataFrame\u001b[39m = [device_id: string, country_code: string ... 12 more fields]\r\n",
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [device_id: string, country_code: string ... 8 more fields]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val frame = spark.read.option(\"header\", true).option(\"inferSchema\",true).csv(\"picsart_2.csv\")\n",
    "val df = frame.drop(\"advertising_id\", \"timezone\", \"duration\", \"app\", \"v\").withColumn(\"date\", frame(\"timestamp\").cast(DateType))\n",
    "df.printSchema()\n",
    "df.select(\"event_type\").show(100,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+--------+\n",
      "|      date|count_id_set|count_ret_ids|ret_rate|\n",
      "+----------+------------+-------------+--------+\n",
      "|2018-02-02|          94|           37|   39.36|\n",
      "|2018-02-03|          41|           23|    56.1|\n",
      "|2018-02-04|          39|           25|    64.1|\n",
      "|2018-02-05|          45|           24|   53.33|\n",
      "|2018-02-06|          35|           22|   62.86|\n",
      "|2018-02-07|          34|           20|   58.82|\n",
      "|2018-02-08|          36|           15|   41.67|\n",
      "|2018-02-09|          29|           16|   55.17|\n",
      "|2018-02-10|          31|           17|   54.84|\n",
      "|2018-02-11|          28|           19|   67.86|\n",
      "|2018-02-12|          33|           23|    69.7|\n",
      "|2018-02-13|          29|           22|   75.86|\n",
      "|2018-02-14|          40|           19|    47.5|\n",
      "|2018-02-15|          33|           17|   51.52|\n",
      "|2018-02-16|          30|           10|   33.33|\n",
      "|2018-02-17|          20|            2|    10.0|\n",
      "|2018-02-18|           2|           -1|   -50.0|\n",
      "+----------+------------+-------------+--------+\n",
      "\n",
      "+------------------+\n",
      "|     avg(ret_rate)|\n",
      "+------------------+\n",
      "|52.626250000000006|\n",
      "+------------------+\n",
      "\n",
      "+----------+------------+-------------+--------+\n",
      "|      date|count_id_set|count_ret_ids|ret_rate|\n",
      "+----------+------------+-------------+--------+\n",
      "|2018-02-02|           8|            2|    25.0|\n",
      "|2018-02-03|           6|            2|   33.33|\n",
      "|2018-02-04|           5|            2|    40.0|\n",
      "|2018-02-05|           5|            1|    20.0|\n",
      "|2018-02-06|           2|            1|    50.0|\n",
      "|2018-02-07|           4|            2|    50.0|\n",
      "|2018-02-08|           5|            1|    20.0|\n",
      "|2018-02-09|           3|            1|   33.33|\n",
      "|2018-02-10|           5|            1|    20.0|\n",
      "|2018-02-11|           2|            0|     0.0|\n",
      "|2018-02-12|           1|            0|     0.0|\n",
      "|2018-02-13|           3|            1|   33.33|\n",
      "|2018-02-14|           3|            1|   33.33|\n",
      "|2018-02-15|           3|            0|     0.0|\n",
      "|2018-02-16|           4|            0|     0.0|\n",
      "|2018-02-17|           1|           -1|  -100.0|\n",
      "+----------+------------+-------------+--------+\n",
      "\n",
      "+------------------+\n",
      "|     avg(ret_rate)|\n",
      "+------------------+\n",
      "|23.887999999999995|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mjava.sql.Date\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.{DateType, IntegerType}\n",
       "\u001b[39m\r\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mRetention\u001b[39m\r\n",
       "\u001b[36mret_1\u001b[39m: \u001b[32mRetention\u001b[39m = ammonite.$sess.cmd49$Helper$Retention@4e5241df"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**Retention object that takes the following arguments: dataframe, date column, id column, and an integer as the number \n",
    "of days. Has two methods: 1) creates a dataframe returning each day's number of users, the number of users that returned back \n",
    "after the number of days indicated in the arguments, and the retention rate, 2) ret_by_event method which takes extra\n",
    "arguments: the filtering column name and the exact filtering value, to return the retention rate of only that event's users.\n",
    "*/\n",
    "\n",
    "class Retention (frame:DataFrame, col_date:String, col_id:String, days:Int) {\n",
    "def day_retention():DataFrame= {\n",
    "  var df_1 = frame.select(col_date, col_id)\n",
    "  var df_2 = df_1.groupBy(col_date).agg(collect_set(col(col_id)).as(\"id_set\"))\n",
    "  var df_join = df_1.join(df_2, df_1(col_date)<=>df_2(col_date), \"inner\").drop(df_1(col_date)).\n",
    "   groupBy(col(col_date), col(\"id_set\")).agg(countDistinct(col(col_id)).as(\"count_id_set\")).sort(col(col_date)) \n",
    "  var window_spec = Window.orderBy(col_date)\n",
    "  var df_join_added = df_join.withColumn(\"id_set_days\", lead(\"id_set\", 1) over window_spec_1)\n",
    "  var df_join_added_1 = df_join_added.withColumn(\"common_id_set\", array_intersect(col(\"id_set\"), col(\"id_set_days\")))\n",
    "  var df_final = df_join_added_1.withColumn(\"count_ret_ids\", size(col(\"common_id_set\"))).withColumn(\"ret_rate\", \n",
    "                 round(col(\"count_ret_ids\")*100/col(\"count_id_set\"), 2)).drop(\"id_set\", \"id_set_days\", \"common_id_set\")\n",
    "                \n",
    "                        \n",
    "  df_final\n",
    "}\n",
    "def ret_by_event(col_event:String, event_type:String): DataFrame={\n",
    "  var df_1 = frame.filter(col(col_event)===event_type).select(col_date, col_id)\n",
    "  var df_2 = df_1.groupBy(col_date).agg(collect_set(col(col_id)).as(\"id_set\"))\n",
    "  var df_join = df_1.join(df_2, df_1(col_date)<=>df_2(col_date), \"inner\").drop(df_1(col_date)).\n",
    "   groupBy(col(col_date), col(\"id_set\")).agg(countDistinct(col(col_id)).as(\"count_id_set\")).sort(col(col_date)) \n",
    "  var window_spec = Window.orderBy(col_date)\n",
    "  var df_join_added = df_join.withColumn(\"id_set_days\", lead(\"id_set\", 1) over window_spec_1)\n",
    "  var df_join_added_1 = df_join_added.withColumn(\"common_id_set\", array_intersect(col(\"id_set\"), col(\"id_set_days\")))\n",
    "  var df_final = df_join_added_1.withColumn(\"count_ret_ids\", size(col(\"common_id_set\"))).withColumn(\"ret_rate\", \n",
    "                 round(col(\"count_ret_ids\")*100/col(\"count_id_set\"), 2)).drop(\"id_set\", \"id_set_days\", \"common_id_set\")\n",
    "                        \n",
    "  df_final\n",
    "}\n",
    " \n",
    "}\n",
    "//1.Calculate how many users who used app in X day:  a) returned the next day  b) returned 7 day after\n",
    "val ret_1 = new Retention(df, \"date\", \"device_id\", 1)  //b) input 7 instead of 1 in the arguments\n",
    "ret_1.day_retention.show()\n",
    "ret_1.day_retention.filter(col(\"date\")=!=\"2018-02-18\").agg(avg(col(\"ret_rate\"))).show()\n",
    "//2.which tool has highest return rate (edit_text_apply, effect_apply, edit_item_open, edit_sticker_apply)\n",
    "ret_1.ret_by_event(\"event_type\", \"edit_text_apply\").show()\n",
    "ret_1.ret_by_event(\"event_type\", \"edit_text_apply\").filter(col(\"date\")=!=\"2018-02-17\").agg(avg(col(\"ret_rate\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+\n",
      "|week_day|      date|device_id|\n",
      "+--------+----------+---------+\n",
      "|       6|2018-02-03|        3|\n",
      "|       7|2018-02-04|        5|\n",
      "|       1|2018-02-05|        1|\n",
      "|       2|2018-02-06|        5|\n",
      "|       3|2018-02-07|        4|\n",
      "|       4|2018-02-08|        3|\n",
      "|       5|2018-02-09|        1|\n",
      "|       6|2018-02-10|        4|\n",
      "|       7|2018-02-11|        1|\n",
      "|       1|2018-02-12|        1|\n",
      "|       2|2018-02-13|        7|\n",
      "|       3|2018-02-14|        4|\n",
      "|       4|2018-02-15|        2|\n",
      "|       5|2018-02-16|        5|\n",
      "|       6|2018-02-17|        4|\n",
      "|       7|2018-02-18|        5|\n",
      "|       1|2018-02-19|        1|\n",
      "|       2|2018-02-20|        7|\n",
      "|       3|2018-02-21|        4|\n",
      "|       4|2018-02-22|        2|\n",
      "|       5|2018-02-23|        1|\n",
      "|       6|2018-02-24|        3|\n",
      "|       1|2018-02-26|        3|\n",
      "|       2|2018-02-27|        4|\n",
      "|       3|2018-02-28|        1|\n",
      "|       4|2018-03-01|        2|\n",
      "|       5|2018-03-02|        2|\n",
      "+--------+----------+---------+\n",
      "\n",
      "+--------+------------------------+\n",
      "|week_day|round(avg(device_id), 2)|\n",
      "+--------+------------------------+\n",
      "|       1|                     1.5|\n",
      "|       2|                    5.75|\n",
      "|       3|                    3.25|\n",
      "|       4|                    2.25|\n",
      "|       5|                    2.25|\n",
      "|       6|                     3.5|\n",
      "|       7|                    3.67|\n",
      "+--------+------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mGroupCount\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfind_avg\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfind_avg_group\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfind_max\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfind_max_group\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfind_min\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfind_min_group\u001b[39m\r\n",
       "\u001b[36mdf_new\u001b[39m: \u001b[32mDataFrame\u001b[39m = [country_code: string, device_id: string ... 9 more fields]\r\n",
       "\u001b[36mdf_1\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [week_day: string, date: date ... 1 more field]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**functions to group_by and count, find average, min, max of a column, and the same for each group\n",
    "*/\n",
    "\n",
    "def GroupCount(frame:DataFrame, groupCols:Array[String], countCols:Array[String]):DataFrame = {\n",
    "    var groupCols1:Array[Column] = groupCols.map(x=>col(x))\n",
    "    var countCols1:Array[Column] = countCols.map(c => countDistinct(col(c)).alias(c))\n",
    "    var dfFiltered = frame.groupBy(groupCols.head, groupCols.tail : _*).agg(countCols1.head, countCols1.tail : _*)\n",
    "    dfFiltered\n",
    "    }\n",
    "    \n",
    "def find_avg(frame:DataFrame, avg_col:String):DataFrame = {\n",
    "    var dfAvgCol = frame.agg(round(avg(col(avg_col)), 2))\n",
    "    dfAvgCol\n",
    "  }\n",
    "\n",
    "def find_avg_group(frame:DataFrame, group_col:String, avg_col:String) = {\n",
    "    var dfAvgColGroup = frame.groupBy(col(group_col)).agg(round(avg(col(avg_col)), 2))\n",
    "    dfAvgColGroup\n",
    "}\n",
    "    \n",
    "def find_max(frame:DataFrame, max_col:String):DataFrame = {\n",
    "     var dfMaxCol = frame.agg(max(col(max_col)))\n",
    "     dfMaxCol\n",
    "  }\n",
    "\n",
    "def find_max_group(frame:DataFrame, group_col:String, max_col:String) = {\n",
    "    var dfMaxColGroup = frame.groupBy(col(group_col)).agg(max(col(max_col)))\n",
    "    dfMaxColGroup\n",
    "}\n",
    "def find_min(frame:DataFrame, min_col:String):DataFrame = {\n",
    "     var dfMinCol = frame.agg(min(col(min_col)))\n",
    "     dfMinCol\n",
    "  }\n",
    "\n",
    "def find_min_group(frame:DataFrame, group_col:String, min_col:String) = {\n",
    "    var dfMinColGroup = frame.groupBy(col(group_col)).agg(min(col(min_col)))\n",
    "    dfMinColGroup\n",
    "}\n",
    "\n",
    "\n",
    "//3) a) how much users did photo_like daily, b) in which day is it highest, c) if there is seasonality\n",
    "val df_new = df.filter(col(\"event_type\")===\"photo_like\").withColumn(\"week_day\", date_format(col(\"timestamp\"), \"u\")) \n",
    "//a)\n",
    "val df_1 = GroupCount(df_new, Array(\"week_day\", \"date\"), Array(\"device_id\")).filter(col(\"date\")=!=\"2018-02-02\").sort(\"date\")\n",
    "df_1.show(50)\n",
    "find_avg_group(df_1, \"week_day\", \"device_id\").sort(col(\"week_day\")).show()\n",
    "//b) in Feb 2018 on average on Tuesday users were more active in liking photos than on any other day.\n",
    "//c)on Saturdays and Sundays, on average, more users did \"photo_like\", especially on Sunday because our data range contains\n",
    "//one Sunday less than any other weekday. And on Mondays definitely the average number of users doing \"photo_like\" \n",
    "//is apparently low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|          event_type|       session_id|\n",
      "+--------------------+-----------------+\n",
      "|          effect_try|              298|\n",
      "|effect_try , effe...|              243|\n",
      "|     conversion_rate|81.54362416107382|\n",
      "+--------------------+-----------------+\n",
      "\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mconversion_rate\u001b[39m"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//4) what is conversion rate that users who did effect_try then did effect apply\n",
    " def conversion_rate(frame:DataFrame, event_col:String, stage1:String, stage2:String, id_col:String, \n",
    "                     time_col:String):Unit = {\n",
    "     val window_spec = Window.partitionBy(id_col).orderBy(time_col)\n",
    "     val count_stage1 = GroupCount(frame.select(id_col, time_col, event_col).filter(col(event_col)=== stage1),\n",
    "                         Array(event_col), Array(id_col))\n",
    "     val df_event = frame.select(id_col, time_col, event_col).filter(col(event_col)=== stage1 || col(event_col)===stage2).\n",
    "                    withColumn(\"lead\", lead(col(event_col), 1) over window_spec).filter(col(event_col)===stage1).\n",
    "                    withColumn(\"concat_col\", concat(col(event_col),lit(\" , \"),col(\"lead\")))\n",
    "     val count_stage2 = GroupCount(df_event.filter(col(\"concat_col\")===stage1+\" , \"+stage2), Array(\"concat_col\"),\n",
    "                                   Array(id_col))\n",
    "     val count_union = count_stage1.union(count_stage2)\n",
    "     val new_df = count_union.union(List((\"conversion_rate\", (count_stage2.select(id_col).\n",
    "                take(1)(0)(0).toString.toDouble*100/count_stage1.select(id_col).take(1)(0)(0).toString.toInt).toString))\n",
    "                                        .toDF)\n",
    "     \n",
    "     println(new_df.show())\n",
    " }\n",
    "conversion_rate(df, \"event_type\", \"effect_try\", \"effect_apply\", \"session_id\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|  event_type|device_id|\n",
      "+------------+---------+\n",
      "|effect_apply|       50|\n",
      "+------------+---------+\n",
      "\n",
      "+------------------------+\n",
      "|round(avg(device_id), 2)|\n",
      "+------------------------+\n",
      "|                    50.0|\n",
      "+------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_5\u001b[39m: \u001b[32mDataFrame\u001b[39m = [event_type: string, device_id: string]\r\n",
       "\u001b[36mdf_5_short\u001b[39m: \u001b[32mDataFrame\u001b[39m = [event_type: string, device_id: bigint]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//5) find how many users did effect apply and what is descriptive statistics for it (outliers, avg, median)\n",
    "val df_5 = df.filter(col(\"event_type\")===\"effect_apply\").select(\"event_type\", \"device_id\" )\n",
    "val df_5_short = GroupCount(df_5, Array(\"event_type\"), Array(\"device_id\"))\n",
    "//count, avg\n",
    "df_5_short.show()\n",
    "find_avg(df_5_short, \"device_id\").show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# outlier formula: outlier<lowerlevel[Q1-IQR*1.5] outlier>higher level[Q3+IQR*1.5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|              events|percent_users|\n",
      "+--------------------+-------------+\n",
      "|                null|        100.0|\n",
      "|interstitial_ad_r...|         9.41|\n",
      "|banner_ad_request...|         7.47|\n",
      "|interstitial_ad_r...|         7.42|\n",
      "|banner_ad_request...|         7.42|\n",
      "|app_load,intersti...|         7.42|\n",
      "|explore_open,app_...|         7.21|\n",
      "|native_ad_waterfa...|         6.41|\n",
      "|native_ad_respons...|         6.41|\n",
      "|interstitial_ad_r...|         5.15|\n",
      "|native_ad_respons...|          5.1|\n",
      "|app_open,explore_...|         5.02|\n",
      "|native_ad_request...|         5.02|\n",
      "|explore_open,inte...|         4.85|\n",
      "|shop_response,sho...|         4.81|\n",
      "|effect_try,effect...|         4.72|\n",
      "|banner_ad_request...|         4.39|\n",
      "|native_ad_respons...|         4.26|\n",
      "|shop_request,shop...|         4.22|\n",
      "|shop_response,sho...|         4.01|\n",
      "+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">window</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">expressions</span></span>.<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">WindowSpec</span></span> = org.apache.spark.sql.expressions.WindowSpec@665c3850</code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mwindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@665c3850"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//6)* what first 5 events users do in their session start (should be used window functions, \n",
    "//find about it in the shared link above) - 5 events in array and % of users did it\n",
    "//No general function yet.\n",
    "var window = Window.partitionBy(\"session_id\").orderBy(\"timestamp\")\n",
    "df.\n",
    "withColumn(\"ev_1\", lead(\"event_type\", 1) over window).\n",
    "withColumn(\"ev_2\", lead(\"event_type\", 2) over window).\n",
    "withColumn(\"ev_3\", lead(\"event_type\", 3) over window).\n",
    "withColumn(\"ev_4\", lead(\"event_type\", 4) over window).\n",
    "withColumn(\"events\",concat(col(\"event_type\"),lit(\",\"),col(\"ev_1\"),lit(\",\"),col(\"ev_2\"),lit(\",\"),col(\"ev_3\"),lit(\",\"),\n",
    "        col(\"ev_4\"))).select(\"session_id\",\"events\").groupBy(\"events\").agg(countDistinct(\"session_id\").as(\"count_5s\")).\n",
    "        orderBy(col(\"count_5s\").desc).withColumn(\"percent_users\",round(col(\"count_5s\")/df.agg(countDistinct(\"session_id\")).\n",
    "        take(1)(0)(0).toString.toInt*100,2)).drop(\"count_5s\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//7)* What are the most important metrics when we want to describe one session of user in app (edited)\n",
    "-average session duration\n",
    "-conversion rate of \"event_try\"/\"event_apply\"\n",
    "-ranking of most used events\n",
    "-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
