{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.Properties\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.DataFrame\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{DataFrame, SQLContext}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.{SparkConf, SparkContext}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{DataFrame, SparkSession}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SaveMode._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.json4s._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.json4s.jackson.JsonMethods._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\r\n",
       "\u001b[36mformatter\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mjoda\u001b[39m.\u001b[32mtime\u001b[39m.\u001b[32mformat\u001b[39m.\u001b[32mDateTimeFormatter\u001b[39m = org.joda.time.format.DateTimeFormatter@5fc90c49\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Try\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.sql.{Connection, DriverManager, ResultSet}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkContext  \n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Column\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.{DataType, DateType, TimestampType}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\r\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@6382f864\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msc\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.3` \n",
    "import org.apache.spark.sql._\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import org.joda.time.{DateTime, Days}\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import java.util.Properties\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.{DataFrame, SQLContext}\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.joda.time.{DateTime, Days}\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.SaveMode._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.json4s._\n",
    "import org.json4s.jackson.JsonMethods._\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import org.joda.time.{DateTime, Days}\n",
    "val formatter = DateTimeFormat.forPattern(\"yyyy-MM-dd\")\n",
    "import scala.util.Try\n",
    "import java.sql.{Connection, DriverManager, ResultSet}\n",
    "import org.apache.spark.SparkContext  \n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.Column\n",
    "import org.apache.spark.sql.types.{DataType, DateType, TimestampType}\n",
    "import org.apache.spark.sql.functions._\n",
    "val spark = {\n",
    "  SparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "def sc = spark.sparkContext\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- language_code: string (nullable = true)\n",
      " |-- platform: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mframe1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [device_id: string, country_code: string ... 12 more fields]\r\n",
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [device_id: string, country_code: string ... 6 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val frame1 = spark.read.option(\"header\", true).option(\"inferSchema\",true).csv(\"picsart_2.csv\")\n",
    "val df = frame1.drop(\"advertising_id\", \"timezone\", \"app\", \"version\", \"duration\", \"v\")\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.math\n",
       "\n",
       "\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mgroup_count\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mDAU\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mavg_DAU\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mMAU\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mavg_MAU\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mYAU\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mavg_YAU\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mstickiness\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mengagement\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mavg_session_duration\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mretention\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mconversion_rate\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmost_popular_five_consecutive_events\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mevent_description\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmost_popular_ten_events\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmost_used_tools_combinations\u001b[39m"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.math\n",
    "\n",
    "def group_count(dataframe:DataFrame, group_col:Array[String], count:Array[String]):DataFrame = {\n",
    "        var count_col:Array[Column] = count.map(c => countDistinct(col(c)).as(\"count_\"+c))\n",
    "        var grouped_frame = dataframe.groupBy(group_col.head, group_col.tail:_*).agg(count_col.head, count_col.tail:_*)\n",
    "        grouped_frame\n",
    "    }\n",
    "def DAU(dataframe:DataFrame, day:String):Int = {\n",
    "        var frame = dataframe.withColumn(\"date\", to_date(col(\"timestamp\")))\n",
    "        var date_col = frame(\"date\")\n",
    "        var device_id_col = frame(\"device_id\")\n",
    "        var dau = frame.filter(date_col===day).agg(countDistinct(device_id_col)).take(1)(0)(0).toString.toInt\n",
    "        dau\n",
    "    }\n",
    "def avg_DAU(dataframe:DataFrame):Double = {\n",
    "        var frame = dataframe.withColumn(\"date\", to_date(col(\"timestamp\"))).filter(col(\"device_id\").isNotNull)\n",
    "        var avg_DAU_frame = group_count(frame, Array(\"date\"), Array(\"device_id\")).\n",
    "                            agg(round(avg(col(\"count_device_id\"))))  \n",
    "        var avg_dau = avg_DAU_frame.take(1)(0)(0).toString.toDouble\n",
    "        avg_dau\n",
    "    }\n",
    "def MAU(dataframe:DataFrame, month_num:String):Int = {\n",
    "        var frame = dataframe.withColumn(\"date\", to_date(dataframe(\"timestamp\"))).\n",
    "                       withColumn(\"month\", month(dataframe(\"timestamp\"))).filter(col(\"device_id\").isNotNull)                \n",
    "        var month_col = frame(\"month\")\n",
    "        var device_id_col = frame(\"device_id\")\n",
    "        var mau = frame.filter(month_col===month_num).agg(countDistinct(device_id_col)).take(1)(0)(0).toString.toInt\n",
    "        mau\n",
    "    }\n",
    "def avg_MAU(dataframe:DataFrame):Double = {\n",
    "        var frame = dataframe.withColumn(\"date\", to_date(dataframe(\"timestamp\"))).\n",
    "                    withColumn(\"month\", month(dataframe(\"timestamp\"))).filter(col(\"device_id\").isNotNull)\n",
    "        var avg_MAU_frame = group_count(frame, Array(\"month\"), Array(\"device_id\")).\n",
    "                            agg(round(avg(col(\"count_device_id\"))))  \n",
    "        var avg_mau = avg_MAU_frame.take(1)(0)(0).toString.toDouble\n",
    "        avg_mau\n",
    "    }\n",
    "def YAU(dataframe:DataFrame, year_num:String):Int = {\n",
    "        var frame = dataframe.withColumn(\"date\", to_date(dataframe(\"timestamp\"))).\n",
    "                    withColumn(\"month\", month(dataframe(\"timestamp\"))).withColumn(\"year\", year(dataframe(\"timestamp\")))\n",
    "                    .filter(col(\"device_id\").isNotNull)\n",
    "        var year_col = frame(\"year\")\n",
    "        var device_id_col = frame(\"device_id\")\n",
    "        var yau = frame.filter(year_col===year_num).agg(countDistinct(device_id_col)).take(1)(0)(0).toString.toInt\n",
    "        yau\n",
    "    }\n",
    "def avg_YAU(dataframe:DataFrame):Double = {\n",
    "        var frame = dataframe.withColumn(\"date\", to_date(dataframe(\"timestamp\"))).\n",
    "                    withColumn(\"month\", month(dataframe(\"timestamp\"))).withColumn(\"year\", year(dataframe(\"timestamp\")))\n",
    "                    .filter(col(\"device_id\").isNotNull)\n",
    "        var avg_YAU_frame = group_count(frame, Array(\"year\"), Array(\"device_id\")).\n",
    "                            agg(round(avg(col(\"count_device_id\"))))  \n",
    "        var avg_yau = avg_YAU_frame.take(1)(0)(0).toString.toDouble\n",
    "        avg_yau\n",
    "    }\n",
    "def stickiness(dataframe:DataFrame):Double = {\n",
    "        var frame = dataframe.withColumn(\"date\", to_date(dataframe(\"timestamp\"))).\n",
    "                    withColumn(\"month\", month(dataframe(\"timestamp\"))).filter(col(\"device_id\").isNotNull)\n",
    "        var stickiness_raw = avg_DAU(frame)*100/avg_MAU(frame)       \n",
    "        var stickiness = scala.math.round(stickiness_raw)\n",
    "        stickiness\n",
    "    } \n",
    "def engagement(dataframe:DataFrame):Double = {\n",
    "        var frame = dataframe.withColumn(\"date\", to_date(dataframe(\"timestamp\"))).filter(col(\"session_id\").isNotNull)\n",
    "        var avg_dau = avg_DAU(frame)\n",
    "        var frame_1 = group_count(frame, Array(\"date\"), Array(\"session_id\")).\n",
    "                      agg(round(avg(col(\"count_session_id\"))))\n",
    "        var sessions_num = frame_1.take(1)(0)(0).toString.toDouble\n",
    "        var avg_sessions_per_dau = scala.math.round(sessions_num/avg_dau)\n",
    "        avg_sessions_per_dau\n",
    "    }\n",
    "def avg_session_duration(dataframe:DataFrame):Double = {\n",
    "        var frame = dataframe.withColumn(\"date\", to_date(dataframe(\"timestamp\"))).filter(col(\"session_id\").isNotNull)\n",
    "        var session_id_col = frame(\"session_id\")\n",
    "        var time_col = frame(\"timestamp\")\n",
    "        var frame_filtered = frame.select(\"session_id\", \"timestamp\").distinct()\n",
    "        var new_frame = frame_filtered.groupBy(session_id_col).agg(min(time_col).as(\"session_start\"), max(time_col).\n",
    "                        as(\"session_end\")).withColumn(\"duration_seconds\", unix_timestamp(col(\"session_end\"))-\n",
    "                        unix_timestamp(col(\"session_start\"))) \n",
    "        var avg_duration = new_frame.agg(round(avg(col(\"duration_seconds\")))).take(1)(0)(0).toString.toDouble\n",
    "   \n",
    "       avg_duration\n",
    "        \n",
    "   }\n",
    "\n",
    "def retention(dataframe:DataFrame, number_of_days:Int):Unit = {\n",
    "        var frame = dataframe.withColumn(\"date\", to_date(dataframe(\"timestamp\"))).filter(col(\"device_id\").isNotNull)\n",
    "        var device_id_col = frame(\"device_id\")\n",
    "        var date_col = frame(\"date\")\n",
    "        var df_1 = group_count(frame, Array(\"date\"), Array(\"device_id\")).withColumnRenamed(\"date\", \"date1\")\n",
    "        var df_2 = frame.select(\"date\", \"device_id\").groupBy(date_col).agg(collect_set(device_id_col).as(\"device_id_set\"))\n",
    "        var df_join = df_1.join(df_2, df_1(\"date1\") <=> df_2(\"date\"), \"inner\")\n",
    "        var window_spec = Window.orderBy(date_col)\n",
    "        var df_3 = df_join.withColumn(\"set_after_days\", lead(\"device_id_set\", number_of_days) over window_spec)\n",
    "        var df_4 = df_3.withColumn(\"common_id_set\", array_intersect(col(\"device_id_set\"), col(\"set_after_days\")))\n",
    "        var df_5 = df_4.withColumn(\"count_ret_ids\", size(col(\"common_id_set\"))).withColumn(\"retention_rate\",\n",
    "                    round(col(\"count_ret_ids\")*100/col(\"count_device_id\"), 2)).drop(\"date1\", \"device_id_set\", \n",
    "                    \"set_after_days\", \"common_id_set\")\n",
    "        var avg_ret_rate = df_5.agg(round(avg(col(\"retention_rate\")), 1).as(\"avg_retention_rate_over_time_period\"))\n",
    "        println(avg_ret_rate.show())\n",
    "    }\n",
    "\n",
    "def conversion_rate(dataframe:DataFrame, stage1:String, stage2:String):Unit = {\n",
    "     var frame = dataframe.select(\"event_type\", \"session_id\", \"timestamp\").filter(col(\"session_id\").isNotNull)\n",
    "     var event_col = frame(\"event_type\")\n",
    "     var window_spec = Window.partitionBy(\"session_id\").orderBy(\"timestamp\")\n",
    "     var count_stage_1 = group_count(frame.filter(event_col === stage1), Array(\"event_type\"), Array(\"session_id\")).\n",
    "                         take(1)(0)(1).toString.toDouble\n",
    "     val df_event = frame.filter(event_col === stage1 || event_col === stage2).\n",
    "                    withColumn(\"lead\", lead(event_col, 1) over window_spec).filter(event_col === stage1).\n",
    "                    withColumn(\"concat_col\", concat(event_col, lit(\" , \"), col(\"lead\")))\n",
    "     val count_stage_2 = group_count(df_event.filter(col(\"concat_col\") === stage1+\" , \"+stage2), \n",
    "                       Array(\"concat_col\"), Array(\"session_id\")).take(1)(0)(1).toString.toInt\n",
    "     val conversion_rate = count_stage_2*100/count_stage_1\n",
    "                                      \n",
    "     println(count_stage_1)\n",
    "     println(count_stage_2)\n",
    "     println(\"conversion rate: \"+conversion_rate)\n",
    " }\n",
    "\n",
    "def most_popular_five_consecutive_events(dataframe:DataFrame):Unit = {\n",
    "        \n",
    "        var window = Window.partitionBy(\"session_id\").orderBy(\"timestamp\")\n",
    "        var new_df = dataframe.filter(col(\"session_id\").isNotNull).\n",
    "        withColumn(\"event_2\", lead(\"event_type\", 1) over window).withColumn(\"event_3\", lead(\"event_type\", 2) over window).\n",
    "        withColumn(\"event_4\", lead(\"event_type\", 3) over window).withColumn(\"event_5\", lead(\"event_type\", 4) over window).\n",
    "        withColumn(\"5_events\", concat(col(\"event_type\"),lit(\",\"),col(\"event_2\"),lit(\",\"),col(\"event_3\"),\n",
    "        lit(\",\"),col(\"event_4\"),lit(\",\"),col(\"event_5\"))).select(\"session_id\",\"5_events\").groupBy(\"5_events\").\n",
    "        agg(countDistinct(\"session_id\").as(\"number_of_users\")).orderBy(col(\"number_of_users\").desc).\n",
    "        withColumn(\"percent_of_all_users\",round(col(\"number_of_users\")/dataframe.agg(countDistinct(\"session_id\")).\n",
    "        take(1)(0)(0).toString.toInt*100,2)).show(7, false)\n",
    "    }\n",
    "def event_description(dataframe:DataFrame, new_event:String):Unit = {\n",
    "        var frame = dataframe.withColumn(\"date\", to_date(dataframe(\"timestamp\"))).filter(col(\"device_id\").isNotNull)\n",
    "        var event_col = frame(\"event_type\")\n",
    "        var frame_1 = frame.filter(event_col === new_event)\n",
    "        var frame_2 = group_count(frame_1, Array(\"date\"), Array(\"device_id\"))\n",
    "        var description = frame_2.describe()\n",
    "        var quantiles = frame_2.stat.approxQuantile(\"count_device_id\", Array(0.25, 0.5, 0.75), 0.0)\n",
    "        var Q1 = quantiles(0)\n",
    "        var median = quantiles(1)\n",
    "        var Q3 = quantiles(2)\n",
    "        var IQR = Q3-Q1\n",
    "        var lower_outlier_range = Q1 - 1.5*IQR\n",
    "        var upper_outlier_range = Q3 + 1.5*IQR\n",
    "        var outliers = frame_2.filter(col(\"count_device_id\")<=lower_outlier_range || col(\"count_device_id\")>= \n",
    "                       upper_outlier_range).withColumnRenamed(\"date\", \"outlier_date\").withColumnRenamed(\"count_device_id\",\n",
    "                       \"outliers\")\n",
    "        println(description.show())\n",
    "        println(\"Q1: \"+Q1+\"\\n\"+\"median: \"+median+\"\\n\"+\"Q3: \"+Q3)\n",
    "        println(outliers.show(30))\n",
    "        \n",
    "    }\n",
    "def most_popular_ten_events(dataframe:DataFrame, time_start:String, time_end:String):Unit = {\n",
    "        var frame = dataframe.withColumn(\"date\", to_date(dataframe(\"timestamp\"))).filter(col(\"session_id\").isNotNull)\n",
    "        var date_col = frame(\"date\")\n",
    "        var frame_1 = frame.filter(date_col>=to_date(lit(time_start)) && date_col<=to_date(lit(time_end)))\n",
    "        var frame_2 = group_count(frame_1, Array(\"event_type\"), Array(\"session_id\")).sort(desc(\"count_session_id\")).limit(10)\n",
    "        println(frame_2.show(false))\n",
    "    }\n",
    "def most_used_tools_combinations(dataframe:DataFrame):Unit = {\n",
    "        var window = Window.partitionBy(\"session_id\").orderBy(\"timestamp\")\n",
    "        var new_df = dataframe.filter(col(\"session_id\").isNotNull).\n",
    "        withColumn(\"tool_2\", lead(\"tool\", 1) over window).\n",
    "        withColumn(\"tools_comb\", concat(col(\"tool\"),lit(\",\"),col(\"tool_2\"))).select(\"session_id\",\"tools_comb\").\n",
    "        groupBy(\"tools_comb\").agg(countDistinct(\"session_id\").as(\"number_of_users\")).orderBy(col(\"number_of_users\").desc).\n",
    "        withColumn(\"percent_of_all_users\",round(col(\"number_of_users\")/dataframe.agg(countDistinct(\"session_id\")).\n",
    "        take(1)(0)(0).toString.toInt*100,2)).show(7, false)\n",
    "    }\n",
    "\n",
    "def filter_function(frame:DataFrame, filter_columns:Array[String], filter_values:Array[String]):Unit = {\n",
    "   var filter_columns_1 = filter_columns.map(c => col(c))\n",
    "   var n = filter_columns.length\n",
    "   var frame2 = frame\n",
    "   for (i <- 0 to n-1) {\n",
    "   frame2 = frame2.filter(filter_columns_1(i)===filter_values(i)) }\n",
    "   println(frame2.show())\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
