{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.Properties\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.DataFrame\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{DataFrame, SQLContext}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.{SparkConf, SparkContext}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{DataFrame, SparkSession}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SaveMode._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.json4s._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.json4s.jackson.JsonMethods._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\r\n",
       "\u001b[36mformatter\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mjoda\u001b[39m.\u001b[32mtime\u001b[39m.\u001b[32mformat\u001b[39m.\u001b[32mDateTimeFormatter\u001b[39m = org.joda.time.format.DateTimeFormatter@4e14d514\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Try\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.sql.{Connection, DriverManager, ResultSet}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkContext  \n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Column\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.{DataType, DateType, TimestampType}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\r\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@56b02fc8\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msc\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.3` \n",
    "import org.apache.spark.sql._\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import org.joda.time.{DateTime, Days}\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import java.util.Properties\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.{DataFrame, SQLContext}\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.joda.time.{DateTime, Days}\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.SaveMode._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.json4s._\n",
    "import org.json4s.jackson.JsonMethods._\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import org.joda.time.{DateTime, Days}\n",
    "val formatter = DateTimeFormat.forPattern(\"yyyy-MM-dd\")\n",
    "import scala.util.Try\n",
    "import java.sql.{Connection, DriverManager, ResultSet}\n",
    "import org.apache.spark.SparkContext  \n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.Column\n",
    "import org.apache.spark.sql.types.{DataType, DateType, TimestampType}\n",
    "import org.apache.spark.sql.functions._\n",
    "val spark = {\n",
    "  SparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "def sc = spark.sparkContext\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- language_code: string (nullable = true)\n",
      " |-- platform: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mframe\u001b[39m: \u001b[32mDataFrame\u001b[39m = [device_id: string, country_code: string ... 12 more fields]\r\n",
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [device_id: string, country_code: string ... 8 more fields]\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mGroupCount\u001b[39m\r\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfind_avg\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val frame = spark.read.option(\"header\", true).option(\"inferSchema\",true).csv(\"picsart_2.csv\")\n",
    "val df = frame.drop(\"advertising_id\", \"timezone\", \"duration\", \"app\", \"v\").withColumn(\"date\", frame(\"timestamp\").cast(DateType))\n",
    "df.printSchema()\n",
    "def GroupCount(frame:DataFrame, groupCols:Array[String], countCols:Array[String]):DataFrame = {\n",
    "    var groupCols1:Array[Column] = groupCols.map(x=>col(x))\n",
    "    var countCols1:Array[Column] = countCols.map(c => countDistinct(col(c)).alias(c))\n",
    "    var dfFiltered = frame.groupBy(groupCols.head, groupCols.tail : _*).agg(countCols1.head, countCols1.tail : _*)\n",
    "    dfFiltered\n",
    "    }\n",
    "def find_avg(frame:DataFrame, avg_col:String):DataFrame = {\n",
    "    var dfAvgCol = frame.agg(round(avg(col(avg_col)), 2))\n",
    "    dfAvgCol\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|  event_type|device_id|\n",
      "+------------+---------+\n",
      "|effect_apply|       50|\n",
      "+------------+---------+\n",
      "\n",
      "+------------------------+\n",
      "|round(avg(device_id), 2)|\n",
      "+------------------------+\n",
      "|                    50.0|\n",
      "+------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_5\u001b[39m: \u001b[32mDataFrame\u001b[39m = [event_type: string, device_id: string]\r\n",
       "\u001b[36mdf_5_short\u001b[39m: \u001b[32mDataFrame\u001b[39m = [event_type: string, device_id: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//5) find how many users did effect apply and what is descriptive statistics for it (outliers, avg, median)\n",
    "val df_5 = df.filter(col(\"event_type\")===\"effect_apply\").select(\"event_type\", \"device_id\" )\n",
    "val df_5_short = GroupCount(df_5, Array(\"event_type\"), Array(\"device_id\"))\n",
    "//count, avg\n",
    "df_5_short.show()\n",
    "find_avg(df_5_short, \"device_id\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier formula: outlier<lowerlevel[Q1-IQR*1.5] outlier>higher level[Q3+IQR*1.5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|              events|percent_users|\n",
      "+--------------------+-------------+\n",
      "|                null|        100.0|\n",
      "|interstitial_ad_r...|         9.41|\n",
      "|banner_ad_request...|         7.47|\n",
      "|interstitial_ad_r...|         7.42|\n",
      "|app_load,intersti...|         7.42|\n",
      "|banner_ad_request...|         7.42|\n",
      "|explore_open,app_...|         7.21|\n",
      "|native_ad_waterfa...|         6.41|\n",
      "|native_ad_respons...|         6.41|\n",
      "|interstitial_ad_r...|         5.15|\n",
      "|native_ad_respons...|          5.1|\n",
      "|native_ad_request...|         5.02|\n",
      "|app_open,explore_...|         5.02|\n",
      "|explore_open,inte...|         4.85|\n",
      "|shop_response,sho...|         4.81|\n",
      "|effect_try,effect...|         4.72|\n",
      "|banner_ad_request...|         4.39|\n",
      "|native_ad_respons...|         4.26|\n",
      "|shop_request,shop...|         4.22|\n",
      "|shop_response,sho...|         4.01|\n",
      "+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">window</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">expressions</span></span>.<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">WindowSpec</span></span> = org.apache.spark.sql.expressions.WindowSpec@586470c2</code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mwindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@586470c2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//6)* what first 5 events users do in their session start (should be used window functions, \n",
    "//find about it in the shared link above) - 5 events in array and % of users did it\n",
    "//No general function yet.\n",
    "var window = Window.partitionBy(\"session_id\").orderBy(\"timestamp\")\n",
    "df.\n",
    "withColumn(\"ev_1\", lead(\"event_type\", 1) over window).\n",
    "withColumn(\"ev_2\", lead(\"event_type\", 2) over window).\n",
    "withColumn(\"ev_3\", lead(\"event_type\", 3) over window).\n",
    "withColumn(\"ev_4\", lead(\"event_type\", 4) over window).\n",
    "withColumn(\"events\",concat(col(\"event_type\"),lit(\",\"),col(\"ev_1\"),lit(\",\"),col(\"ev_2\"),lit(\",\"),col(\"ev_3\"),lit(\",\"),\n",
    "        col(\"ev_4\"))).select(\"session_id\",\"events\").groupBy(\"events\").agg(countDistinct(\"session_id\").as(\"count_5s\")).\n",
    "        orderBy(col(\"count_5s\").desc).withColumn(\"percent_users\",round(col(\"count_5s\")/df.agg(countDistinct(\"session_id\")).\n",
    "        take(1)(0)(0).toString.toInt*100,2)).drop(\"count_5s\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//7)* What are the most important metrics when we want to describe one session of user in app (edited)\n",
    "-session duration (avg session duration in a period of time)\n",
    "-conversion rate of \"event_try\"/\"event_apply\"\n",
    "-ranking of most used events\n",
    "-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
