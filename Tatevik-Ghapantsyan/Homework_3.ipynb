{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "// import $ivy.`sh.almond::almond-spark:_` // Added automatically on importing Spark\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.Properties\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.DataFrame\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{DataFrame, SQLContext}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.{SparkConf, SparkContext}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{DataFrame, SparkSession}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SaveMode._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.json4s._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.json4s.jackson.JsonMethods._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.format.DateTimeFormat\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.joda.time.{DateTime, Days}\n",
       "\u001b[39m\n",
       "\u001b[36mformatter\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mjoda\u001b[39m.\u001b[32mtime\u001b[39m.\u001b[32mformat\u001b[39m.\u001b[32mDateTimeFormatter\u001b[39m = org.joda.time.format.DateTimeFormatter@52227a0e\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Try\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.sql.{Connection, DriverManager, ResultSet}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@d360be5\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msc\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.3` // Or use any other 2.x version here\n",
    "// import $ivy.`sh.almond::almond-spark:_` // Added automatically on importing Spark\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import org.joda.time.{DateTime, Days}\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import java.util.Properties\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.{DataFrame, SQLContext}\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.joda.time.{DateTime, Days}\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.SaveMode._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.json4s._\n",
    "import org.json4s.jackson.JsonMethods._\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import org.joda.time.{DateTime, Days}\n",
    "val formatter = DateTimeFormat.forPattern(\"yyyy-MM-dd\")\n",
    "import scala.util.Try\n",
    "import java.sql.{Connection, DriverManager, ResultSet}\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = {\n",
    "  SparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n",
    "def sc = spark.sparkContext\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">df</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">DataFrame</span></span> = [device_id: string, country_code: string ... 13 more fields]</code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [device_id: string, country_code: string ... 13 more fields]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var df = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(\"Data_02.csv\").\n",
    "        withColumn(\"date\",to_date(col(\"timestamp\"),\"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|N of people who used app after 1 day|\n",
      "+------------------------------------+\n",
      "|                                  69|\n",
      "+------------------------------------+\n",
      "\n",
      "+-------------------------------------+\n",
      "|N of people who used app after 7 days|\n",
      "+-------------------------------------+\n",
      "|                                   49|\n",
      "+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// ----------------------------- Task 1 --------------------------------------------------------------\n",
    "// Calculate how many users who used app in X day \n",
    "// a) returned the next day\n",
    "// b) returned 7 day after\n",
    "\n",
    "val df_1 = df.withColumn(\"date_plus_one\", date_add(col(\"date\"), 1)).\n",
    "            select(col(\"device_id\").as(\"DeviceId\"), col(\"date\").alias(\"Date\"), col(\"date_plus_one\").as(\"DatePlusOne\")).\n",
    "            join(df, \n",
    "                col(\"DeviceId\") === col(\"device_id\") && \n",
    "                col(\"DatePlusOne\") === df.col(\"date\")).\n",
    "            select(col(\"DeviceId\"), df.col(\"date\"), col(\"DatePlusOne\")).\n",
    "            select(countDistinct(\"DeviceId\").as(\"N of people who used app after 1 day\")).\n",
    "            show()\n",
    "\n",
    "\n",
    "val df_2 = df.withColumn(\"date_plus_seven\", date_add(col(\"date\"), 7)).\n",
    "            select(col(\"device_id\").as(\"DeviceId\"), col(\"date\").alias(\"Date\"), col(\"date_plus_seven\").as(\"DatePlusSeven\")).\n",
    "            join(df, \n",
    "                col(\"DeviceId\") === col(\"device_id\") && \n",
    "                col(\"DatePlusSeven\") === df.col(\"date\")).\n",
    "            select(col(\"DeviceId\"), df.col(\"date\"), col(\"DatePlusSeven\")).\n",
    "            select(countDistinct(\"DeviceId\").as(\"N of people who used app after 7 days\")).\n",
    "            show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+\n",
      "|FirstEvent        |count(EventReapeated)|\n",
      "+------------------+---------------------+\n",
      "|edit_text_apply   |21                   |\n",
      "|edit_sticker_apply|10                   |\n",
      "|effect_apply      |39                   |\n",
      "|edit_item_open    |78                   |\n",
      "+------------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">df_2</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Dataset</span></span>[<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Row</span></span>] = [DeviceId: string, SessionId: string ... 2 more fields]</code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mdf_2\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [DeviceId: string, SessionId: string ... 2 more fields]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// ----------------------------- Task 2 --------------------------------------------------------------\n",
    "// which tool has highest return rate (edit_text_apply, effect_apply, edit_item_open, edit_sticker_apply)\n",
    "\n",
    "var df_2 = df.\n",
    "    select(col(\"device_id\").as(\"DeviceId\"), col(\"session_id\").as(\"SessionId\"), col(\"event_type\").as(\"EventType\"), col(\"timestamp\").as(\"Timestamp2\")).\n",
    "    filter($\"EventType\" === \"edit_text_apply\" || $\"EventType\" === \"effect_apply\" || \n",
    "           $\"EventType\" === \"edit_item_open\"  || $\"EventType\" === \"edit_sticker_apply\" )\n",
    "df_2.join(df, col(\"DeviceId\") === col(\"device_id\") && col(\"EventType\") === col(\"event_type\") && col(\"Timestamp2\") < col(\"timestamp\")).\n",
    "    select(col(\"DeviceId\"), col(\"EventType\").as(\"FirstEvent\"), col(\"event_type\").as(\"EventReapeated\")).distinct().\n",
    "    groupBy(col(\"FirstEvent\")).agg(count(\"EventReapeated\")).show(4, false)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|date      |count(device_id)|\n",
      "+----------+----------------+\n",
      "|2018-02-02|7               |\n",
      "|2018-02-03|3               |\n",
      "|2018-02-04|4               |\n",
      "|2018-02-05|3               |\n",
      "|2018-02-06|3               |\n",
      "|2018-02-07|5               |\n",
      "|2018-02-08|2               |\n",
      "|2018-02-09|2               |\n",
      "|2018-02-10|3               |\n",
      "|2018-02-11|1               |\n",
      "|2018-02-12|2               |\n",
      "|2018-02-13|7               |\n",
      "|2018-02-14|3               |\n",
      "|2018-02-15|2               |\n",
      "|2018-02-16|5               |\n",
      "|2018-02-17|5               |\n",
      "|2018-02-18|5               |\n",
      "|2018-02-19|1               |\n",
      "|2018-02-20|6               |\n",
      "|2018-02-21|5               |\n",
      "|2018-02-22|1               |\n",
      "|2018-02-23|2               |\n",
      "|2018-02-24|2               |\n",
      "|2018-02-26|4               |\n",
      "|2018-02-27|3               |\n",
      "|2018-02-28|1               |\n",
      "|2018-03-01|2               |\n",
      "|2018-03-02|2               |\n",
      "+----------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">df_old</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">DataFrame</span></span> = [country_code: string, device_id: string ... 13 more fields]\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">df_3</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Unit</span></span> = ()</code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mdf_old\u001b[39m: \u001b[32mDataFrame\u001b[39m = [country_code: string, device_id: string ... 13 more fields]\n",
       "\u001b[36mdf_3\u001b[39m: \u001b[32mUnit\u001b[39m = ()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// ----------------------------- Task 3 --------------------------------------------------------------\n",
    "// how many users did photo_like daily, in which day is it highest, if there is seasonality\n",
    "\n",
    "var df_old = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(\"Data.csv\").\n",
    "    withColumn(\"date\",to_date(col(\"timestamp\"),\"yyyy-MM-dd\"))\n",
    "var df_3 = df_old.select(col(\"date\"), col(\"device_id\"), col(\"event_type\")).\n",
    "    where(col(\"event_type\") === \"photo_like\").\n",
    "    select(col(\"date\"), col(\"device_id\")).distinct().\n",
    "    groupBy(col(\"date\")).\n",
    "    agg(count(\"device_id\")).as(\"photo_like_users\").\n",
    "    sort(\"date\").\n",
    "    show(30,false)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_4\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [DeviceId: string, EventType: string ... 2 more fields]\n",
       "\u001b[36meff_try_count\u001b[39m: \u001b[32mAny\u001b[39m = \u001b[32m56L\u001b[39m\n",
       "\u001b[36meff_tr_app_count\u001b[39m: \u001b[32mAny\u001b[39m = \u001b[32m50L\u001b[39m"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// ----------------------------- Task 4 --------------------------------------------------------------\n",
    "// what is conversion rate that users who did effect_try then did effect apply\n",
    "\n",
    "val df_4 = df.select(col(\"device_id\").as(\"DeviceId\"), 'event_type.as(\"EventType\"), 'timestamp.as(\"Timestamp1\"), 'date.as(\"Date1\")).\n",
    "    where(col(\"EventType\") === \"effect_try\")\n",
    "val eff_try_count = df_4.select(countDistinct(\"DeviceId\")).take(1)(0)(0)\n",
    "val eff_tr_app_count = df_4.join(df, 'DeviceId === 'device_id && 'event_type === \"effect_apply\" && 'Timestamp1 < 'timestamp && 'Date1 === 'date).\n",
    "    select(countDistinct('DeviceId)).take(1)(0)(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|total_number_of_users|\n",
      "+---------------------+\n",
      "|                   98|\n",
      "+---------------------+\n",
      "\n",
      "+------------------+\n",
      "|effect_apply_users|\n",
      "+------------------+\n",
      "|                50|\n",
      "+------------------+\n",
      "\n",
      "+-----------------+-----------------+-----------------+\n",
      "|max(effect_apply)|min(effect_apply)|avg(effect_apply)|\n",
      "+-----------------+-----------------+-----------------+\n",
      "|149              |1                |13.92            |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "+-----------------------------------------+------+\n",
      "|id                                       |median|\n",
      "+-----------------------------------------+------+\n",
      "|andy-9674bf80-55df-4686-9547-118c87fb6607|4     |\n",
      "+-----------------------------------------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_5\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [device_id: string, country_code: string ... 13 more fields]\n",
       "\u001b[36mcount_ea\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [device_id: string, effect_apply: bigint]\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Row\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
       "\u001b[39m\n",
       "\u001b[36mmedian_rdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mRow\u001b[39m] = ParallelCollectionRDD[1027] at parallelize at cmd110.sc:10\n",
       "\u001b[36mschema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"id\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"median\"\u001b[39m, LongType, true, {})\n",
       ")\n",
       "\u001b[36mmedian_df\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: string, median: bigint]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// ----------------------------- Task 5 --------------------------------------------------------------\n",
    "//find how many users did effect apply and what is descriptive statistics for it (outliers, avg, median)\n",
    "\n",
    "df.select(countDistinct(\"device_id\").as(\"total_number_of_users\")).show(1)\n",
    "val df_5 = df.where(col(\"event_type\") === \"effect_apply\")\n",
    "df_5.select(countDistinct(\"device_id\").as(\"effect_apply_users\")).show(1)\n",
    "val count_ea = df_5.groupBy(\"device_id\").\n",
    "    agg(count(\"event_type\").as(\"effect_apply\")).sort(\"effect_apply\")\n",
    "count_ea.agg(max(\"effect_apply\"), min(\"effect_apply\"), avg(\"effect_apply\")).show(5, false)\n",
    "\n",
    "// this part calculates median\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
    "val median_rdd = spark.sparkContext.parallelize(Seq(count_ea.rdd.take(25).last))\n",
    "val schema = new StructType(Array(new StructField(\"id\", StringType, true), \n",
    "                            new StructField(\"median\", LongType, true)))\n",
    "val median_df = spark.createDataFrame(median_rdd, schema)\n",
    "median_df.show(1, false)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// ----------------------------- Task 6 --------------------------------------------------------------\n",
    "// what first 5 events users do in their session start  - 5 events in array and % of users did it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// ----------------------------- Task 7 --------------------------------------------------------------\n",
    "// What are the most important metrics when we want to describe one session of user in app\n",
    "\n",
    "// 1. how long user stayed in the app (duration or difference of first and last timestamps)\n",
    "// 2. usage of picsart tools\n",
    "// 3. purcheses made by user\n",
    "// 4. is it paid user or free subscribtion user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
